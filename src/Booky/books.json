[
  {
    "cover_image": "https://media.shortform.com/covers/png/chip-war-cover.png",
    "title": "Chip War",
    "author": "Chris Miller",
    "tags": [
      "Politics",
      "Society/Culture",
      "Technology"
    ],
    "url_slug": "chip-war",
    "html": "<p>In <em><a href=\"https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172008\" target=\"_blank\">Chip War</a></em>, economic historian Chris Miller explores the rise of the semiconductor industry. Through his analysis and recounting of this history, Miller shows how semiconductor chips—the crucial components in electronics that control the flow of electric current—became the delivery mechanism for the binary code that powers our digital world and an indispensable input in all electronics. This includes everything from personal computers to household appliances to advanced weapons systems.</p>\n<p>This ubiquity and utility has made chips a linchpin of geopolitical power and a resource that powerful actors have sought to control.</p>\n<p>In this guide, we’ll explore:</p>\n<ul>\n<li>How a handful of researchers and entrepreneurs in the United States founded the modern chip industry in the immediate aftermath of World War II</li>\n<li>How Japanese companies emerged as the primary semiconductor rivals to the US beginning in the 1960s </li>\n<li>How Taiwanese firms—primarily led by giant TSMC—pioneered the foundry model </li>\n<li>How the People’s Republic of China emerged as a new geopolitical rival in the global semiconductor competition in the 21st century</li>\n</ul>\n<p>Throughout the guide, we’ll supplement Miller’s analysis of the history of the chip industry and its geopolitical implications with insights from other experts—including both supporters and critics of Miller.</p>\n<blockquote><p><strong>Critical Reaction to <em>Chip War</em></strong></p>\n<p>When it was published in 2022, <em>Chip War</em> was hailed as a timely narrative of the rise of the chip industry, its crucial role in today’s global economy, and its centrality to geopolitical strategy.</p>\n<p>In its review, <em>The New York Times </em><a href=\"https://www.nytimes.com/2022/10/08/books/review/chip-war-chris-miller.html?login=email&amp;auth=login-email&amp;login=email&amp;auth=login-email\" target=\"_blank\">praised the book for its timeliness, relevance, and extensive research</a> in tackling a complex and technical subject and making it accessible to a general audience. In particular, the <em>Times </em>noted the book’s valuable insights into the geopolitics of the semiconductor industry, particularly in the context of US-China relations—providing readers with an understanding of how technology competition shapes international relations. The review also celebrated the book’s exploration of the vulnerabilities and challenges in the global semiconductor supply chain, a topic that gained prominence when those supply chains were disrupted during the Covid-19 pandemic.</p>\n<p><em>Forbes </em>likewise lauded the book's success in <a href=\"https://www.forbes.com/sites/roslynlayton/2022/11/04/review-of-chip-war-the-fight-for-the-worlds-most-critical-technology/?sh=7f8061c91059\" target=\"_blank\">conveying the geopolitical significance of the global semiconductor industry</a>, especially commending Miller for his in-depth exploration of China's ambitions to achieve self-sufficiency in semiconductor manufacturing. However, the review also noted that some readers might find that the book's emphasis on the US-China rivalry overshadows other important aspects of the semiconductor industry, such as the role of other countries in the complexities of the global chip supply chain.</p>\n</blockquote>\n<h3 id=\"part-1-the-birth-of-semiconductors\">Part 1: The Birth of Semiconductors</h3>\n<p>Miller writes that <strong>the modern chip industry traces its origins to the immediate aftermath of World War II, when a small group of engineers and entrepreneurs realized the vast military potential of semiconductors</strong>. This represented a massive opportunity with the rise of the Cold War in the 1940s and 1950s and the resulting unprecedented demands of the American military for new technology.</p>\n<p>In this section, we’ll explore the Cold War context in which the discovery of semiconductors took place, the invention of the chip, its initial use in military and defense applications, and the rapid growth of the chip industry.</p>\n<h4 id=\"the-cold-war-context\">The Cold War Context</h4>\n<p><strong>The Cold War was the backdrop—and ultimately the impetus—for the emergence of the semiconductor industry.</strong> After the devastation of World War II, writes Miller, the United States and the Soviet Union emerged as the world’s two military and geopolitical superpowers. Over the next half-century, the two nations and their respective allies and satellite states would wage the Cold War—a titanic ideological, economic, and political conflict.</p>\n<p>As Miller notes, the Cold War competition underscored the US military’s need for technology to guide its bombs, missiles, and eventually its satellites with precision. This is what ultimately drove the military’s insatiable appetite for computing power.</p>\n<blockquote><p><strong>The Military Push for New Technology After World War II</strong></p>\n<p>Miller argues that the Cold War was the impetus for the US military’s massive investments in new technology after World War II, but there were other factors as well:</p>\n<p><strong>Technological advancements in warfare</strong>: World War II had demonstrated <a href=\"https://www.nationalww2museum.org/war/articles/scientific-and-technological-advances-world-war-ii\" target=\"_blank\">the strategic importance of technological superiority in warfare</a>. This led to a continuous push for innovations in military technology, including advanced weaponry, communication systems, and surveillance equipment.</p>\n<p><strong>Industrial and economic factors</strong>: The defense industry played a significant role in the post-war US economy. The development and production of advanced military technology provided jobs and economic stimulus, motivating continued investment in defense research and development.</p>\n</blockquote>\n<h4 id=\"discovering-the-power-of-semiconductors\">Discovering the Power of Semiconductors</h4>\n<p>It was in this broader historical context, writes Miller, that scientists discovered how semiconductors could be harnessed to spark a revolution in electronics. <strong>Semiconductors are materials that have electrical conductivity between that of a conductor (a material that allows the flow of electrons with minimal resistance) and an insulator (a material that blocks the flow of electrons)</strong>. They’re crucial components in electronics, as their unique electrical properties can be manipulated to control the flow of electric current.</p>\n<p>In 1947, William Shockley, an employee at Bell Labs, found that by having metal contacts make physical connections with semiconductors, he could create a transistor—a kind of electrical switch that could be turned on or off to regulate or amplify the flow of electricity in electronic circuits.</p>\n<p>(Shortform note: <a href=\"https://www.pbs.org/transistor/album1/addlbios/egos.html\" target=\"_blank\">William Shockley had some controversies with his coworkers over credit for the invention of the transistor</a>. Some sources note that, while Shockley played a role in the theoretical understanding of semiconductors, he came into the project <em>after </em>the initial breakthrough by his colleagues John Bardeen and Walter Brattain. Shockley, however, insisted on being included in the credit for the invention—even though his name wasn’t even on the original patent.)</p>\n<h5 id=\"from-vacuum-tubes-to-transistors\">From Vacuum Tubes to Transistors</h5>\n<p>Miller explains that <strong>the transistor was a game-changer for military—and, later, civilian—electronics</strong>. Up to this point, vacuum tubes were the primary electronic components in military hardware. Vacuum tubes are glass or metal envelopes that contain no air or other gasses inside. The vacuum inside the tube helps prevent the interference of gas particles with the flow of electrons.</p>\n<p>But they were far from ideal. Vacuum tubes were bulky, prone to malfunction, and physically large, limiting their potential, particularly in military contexts. Transistors, on the other hand, were significantly smaller, lighter, and more efficient and durable.</p>\n<p>(Shortform note: While transistors have largely replaced vacuum tubes in most electronic applications due to their smaller size, lower power consumption, and reliability, <a href=\"https://www.engineering.com/story/vacuum-tubes-the-world-before-transistors\" target=\"_blank\">vacuum tubes still find use in certain niche applications</a>. One notable area where vacuum tubes are still used is in high-power amplification, such as in high-fidelity audio amplifiers and guitar amplifiers. Vacuum tubes can provide a warm and natural sound that’s favored by audiophiles and musicians. Vacuum tubes are also employed in radio frequency (RF) and microwave devices like cell phones and Wi-Fi routers, where they can generate high-frequency signals and high-power microwave radiation more efficiently than transistors.)</p>\n<h4 id=\"the-rise-of-the-chip-industry\">The Rise of the Chip Industry</h4>\n<p>Following these discoveries, writes Miller, <strong>the 1950s saw a small group of engineers and entrepreneurs found the modern chip industry.</strong></p>\n<p>In 1955, William Shockley founded the company Shockley Semiconductor in Palo Alto, California. In 1958, details Miller, some of Shockley’s engineers left, founding Fairchild Semiconductor, which would go on to eclipse Shockley Semiconductor in the emerging semiconductor space.</p>\n<p>(Shortform note: Miller notes that Shockley’s engineers chafed under his authoritarian management style and passion for racist eugenics theories. <a href=\"https://www.wired.com/story/silicon-valleys-first-founder-was-its-worst/?redirectURL=https%3A%2F%2Fwww.wired.com%2Fstory%2Fsilicon-valleys-first-founder-was-its-worst%2F\" target=\"_blank\">Some commentators have gone even further, branding Shockley as the original, prototypical misguided Silicon Valley founder</a>. According to this view, Shockley’s erratic and confrontational style was a harbinger of the provocative actions of later tech entrepreneurs, including Elon Musk of Tesla. Shockley’s idiosyncratic behaviors—such as submitting recruits to rigorous psychological tests—and his alienating behavior (including his penchant for practical jokes and trolling) seem to be an early example of the shortcomings seen in some Silicon Valley figures today.)</p>\n<p>In 1958, Jack Kilby at Texas Instruments (TI) invented the integrated circuit, known today as the \"chip.\" <strong>These contained multiple transistors fabricated onto a single physical “chip” of semiconductor material</strong> (usually silicon). Chips took semiconductors to the next level, offering lower power consumption (and thus greater efficiency and durability) and enabling more compact designs.</p>\n<blockquote><p><strong>Who Deserves Credit for the Integrated Circuit?</strong></p>\n<p>Kilby was awarded the Nobel Prize in Physics in 2000 for his contribution to the invention of the integrated circuit. However, <a href=\"https://www.washingtonpost.com/archive/politics/2005/06/22/engineers-tiny-chip-changed-the-world/7e17ba53-c1ea-4c48-878a-83b79d7574e9/\" target=\"_blank\">the Nobel Committee's decision was met with some controversy and criticism</a>.</p>\n<p>Some argued that Kilby's work was not as well-known or as groundbreaking as that of other scientists, like Robert Noyce, who had also made significant contributions to the development of the microchip. Noyce, a cofounder of Intel, had independently developed a similar integrated circuit around the same time as Kilby. Noyce's version, known as the planar integrated circuit, had some advantages over Kilby's design and became more widely adopted in the industry.</p>\n</blockquote>\n<h5 id=\"doing-business-with-the-military\">Doing Business With the Military</h5>\n<p>Miller argues that <strong>the timing could not have been more fortuitous for these early chip entrepreneurs</strong>. With the Cold War-era space race in full swing, the US government was determined to outcompete the Soviets in satellite technology and space exploration. In this context, NASA became a major buyer of Fairchild Semiconductor's chips for guidance systems and satellites.</p>\n<p>At the same time, notes Miller, TI was selling its chips to the US military for guided missile systems. TI’s success was due in no small part to the insights of Chinese-born engineer Morris Chang, whose efforts to refine and perfect the chip manufacturing process enabled TI to master the mass production and miniaturization of chips. Chang would later go on to play a leading role in the rise of the Taiwanese chip industry—a subject we’ll explore in more detail later in this guide.</p>\n<blockquote><p><strong>Defense Spending and the Postwar California Economy</strong></p>\n<p>The military’s investment in semiconductor technology was just one part of a massive influx of defense spending after World War II that played an enormous role in shaping the modern American economy—and in particular, the economy of California, where the chip industry was taking root.</p>\n<p>In <em><a href=\"https://shortform.com/app/book/fast-food-nation\" target=\"_blank\">Fast Food Nation</a></em>, Eric Schlosser notes that in Southern California, defense spending was responsible for $20 billion worth of investment in the region, as the federal government built airplane factories, steel mills, military bases, and naval ports. Indeed, writes Schlosser, <a href=\"https://shortform.com/app/book/fast-food-nation/chapter-1\" target=\"_blank\">federal spending accounted for approximately half of Southern Californians’ personal income</a>.</p>\n<p>This growth would only continue, as <a href=\"https://capitolmuseum.ca.gov/exhibits/called-to-action-californias-role-in-ww2/california-after-the-war/\" target=\"_blank\">California became a hub for the defense industry</a>, with numerous defense contractors and military bases establishing a presence in the state. The aerospace and electronics sectors, in particular, flourished due to government contracts and research initiatives—leading to substantial job creation in California that attracted a diverse workforce from across the country.</p>\n</blockquote>\n<h4 id=\"moores-law-and-the-expansion-to-the-civilian-market\">Moore’s Law and the Expansion to the Civilian Market</h4>\n<p>Miller writes that new thinking about the growth potential of semiconductors fueled the optimism of these chip pioneers. In 1965, Gordon Moore (who, along with Noyce, cofounded the computer chip powerhouse Intel in 1968), predicted that <strong>the number of transistors that could fit on a microchip would <em>double </em>approximately every two years—while the cost per transistor would <em>decrease</em></strong>.</p>\n<p>This principle became known as Moore’s law, and its predictions have largely withstood the test of time for six decades. Miller writes that this had massive implications for the nascent chip industry, as it meant that computers and electronic devices would be able to perform increasingly complex tasks at faster speeds.</p>\n<p>The chip industry’s behavior aligned well with what Moore’s law predicted. Chip manufacturers became focused on cramming more components onto semiconductors, thus exponentially increasing processing power.</p>\n<p>(Shortform note: Although Miller writes that Moore’s law has largely stood the test of time since its promulgation in the mid-1960s, some prominent tech figures have questioned whether it still has predictive power. <strong>In 2022, <a href=\"https://www.marketwatch.com/story/moores-laws-dead-nvidia-ceo-jensen-says-in-justifying-gaming-card-price-hike-11663798618\" target=\"_blank\">NVIDIA CEO Jensen Huang declared that Moore's Law was no longer applicable</a> in its traditional sense</strong>. According to Huang, the doubling of transistor counts on microchips every two years is no longer the sole metric for measuring progress in the semiconductor industry. Instead, today’s advancements are related to innovations in chip architectures, performance improvements, and specialized processing units.)</p>\n<h3 id=\"part-2-the-japanese-challenge\">Part 2: The Japanese Challenge</h3>\n<p>Following the emergence of the chip industry in the US in the 1950s, the early players soon found themselves in a global competition. <strong>As the 1960s and 1970s unfolded, the American chip industry faced new challenges from emerging actors—chiefly Japan</strong>. In this section, we’ll explore how Japanese entrepreneurs began to edge out American chip companies in the consumer electronics space, how the US private and public sectors responded, and how the US began to regain its footing in the 1980s.</p>\n<h4 id=\"the-emergence-of-japan\">The Emergence of Japan</h4>\n<p>As the American chip industry began to take off in the 1950s and 1960s, writes Miller, <strong>Japanese entrepreneurs saw the potential of the technology to create new private fortunes—and transform their war-ravaged country into an economic powerhouse</strong>.</p>\n<p>Visionary figures like Akio Morita, who founded Sony, recognized the potential of semiconductors for the consumer market—not just military applications. Sony started out as a small electronics shop in 1946, but its commitment to innovation and engineering excellence eventually led to the development of many groundbreaking products, including the first transistor radio in 1955 and the first all-transistor television in 1959. These products were more efficient, portable, and durable than their predecessors. The strategy of Japanese companies at this time, like Sony, was to license existing American technology. But they excelled at adapting American chips to use in consumer electronics such as radios, televisions, and handheld calculators.</p>\n<p>Despite the economic challenge Japan represented to American companies, <strong>US geopolitical priorities encouraged Japanese semiconductor growth.</strong> Miller explains that this was because the US wanted an economically strong Japan, which it saw as an important regional counterbalance to and bulwark against the communist People's Republic of China. Thus, the chip industry even in these early years was inextricably linked to geopolitical strategy—a theme we’ll continue to explore throughout this guide.</p>\n<blockquote><p><strong>The Japanese Economic Miracle </strong></p>\n<p>The semiconductor innovations that Miller details in this section occurred during Japan’s remarkable 40-year run of economic growth from the end of World War II to the end of the Cold War. This period is known to economic historians as the “Japanese economic miracle.” <a href=\"https://hbr.org/1998/01/reinterpreting-the-japanese-economic-miracle\" target=\"_blank\">The country’s post-World War II economic success was due to a combination of factors</a> that propelled Japan into becoming an economic powerhouse and provided fertile ground for the country’s nascent semiconductor industry to take root.</p>\n<p><strong>Producer economic state</strong>: Japan adopted a producer economic state model, subordinating other national economic goals to catch up with and potentially surpass the US economy—an approach they sustained for nearly 40 years.</p>\n<p><strong>Government support</strong>: The Japanese government played a role as a business adjunct and referee. It provided incentives like tax breaks, cheap credit, and administrative guidance to steer keiretsu (business networks) toward promising sectors. Other policies, including trade barriers and exchange rate management, also supported and protected Japanese companies.</p>\n<p><strong>American support</strong>: <a href=\"https://www.cambridge.org/core/journals/journal-of-east-asian-studies/article/americas-role-in-the-making-of-japans-economic-miracle/9C7CC6A85CE125290BAD2735B09A882A\" target=\"_blank\">The United States provided financial aid and resources to Japan</a> through programs like the Marshall Plan, which helped Japan rebuild its infrastructure and industries. In addition, the US facilitated trade relations with Japan, providing access to American markets for Japanese goods. This access allowed Japanese industries to export products and generate foreign exchange, which was crucial for economic recovery.</p>\n</blockquote>\n<h4 id=\"the-japanese-advantage\">The Japanese Advantage</h4>\n<p><strong>By the 1980s, writes Miller, Silicon Valley faced a formidable challenge from Japan. </strong>Japanese companies had not only caught up with American technology but were also no longer just <em>licensing </em>American technology—they were <em>producing </em>their own chips, and higher-quality ones at that. Meanwhile, innovations like the Sony Walkman, first introduced in 1979, underscored Japan's prowess in consumer electronics.</p>\n<p>Miller notes that Japanese chip companies also benefited from a confluence of macroeconomic forces that worked in their favor at this time. Thanks to low inflation and a booming economy, Japan had a glut of savings in the late 1970s, which was used to fund investments in new industries like semiconductor chips and consumer electronics. Because there was so much capital, firms could access business loans at low interest rates, fueling quick growth. At the same time, the Japanese government implemented protectionist policies, including tariffs and quotas on foreign chip shipments. This further tilted the playing field in Japan's favor.</p>\n<blockquote><p><strong>Japanese Industrial Policy and the Semiconductor Industry</strong></p>\n<p>Some researchers assert that the Japanese government’s support for its booming semiconductor industry at this time extended well beyond the tariffs and quotas that Miller describes. <strong><a href=\"https://www.csis.org/blogs/perspectives-innovation/japans-semiconductor-industrial-policy-1970s-today\" target=\"_blank\">Japanese industrial policy in the semiconductor industry played a pivotal role in the global market share gain</a></strong> of Japanese semiconductor companies. Notably, there was a significant increase in funding for research and development (R&amp;D) in semiconductor manufacturing equipment, which grew to 26% of Japan's total R&amp;D spending by 1977, up from just 2% at the beginning of the 1970s.</p>\n<p>The Japanese government also invested $300 million in the establishment of the Super LSI Technology Research Association, a public-private technology research project involving Japan's six main computer companies. This collaboration allowed these rival companies to work together and share information, fostering innovation and the development of a common technology platform.</p>\n<p>This close state-private coordination propelled Japanese firms to dominate the global semiconductor market, with Japanese companies accounting for 51% of worldwide sales by 1988.</p>\n</blockquote>\n<h4 id=\"the-american-response\">The American Response</h4>\n<p>Miller writes that, in the face of this challenge, <strong>American semiconductor CEOs realized they needed both government support and new technologies to regain their former position. </strong></p>\n<p>Advocating that chips were a strategic resource vital to American military and economic security, semiconductor industry heads founded Semiconductor Manufacturing Technology (SEMATECH) in 1987. SEMATECH was a consortium of semiconductor manufacturers, equipment suppliers, and the US government. Its primary mission was to advance semiconductor manufacturing technology and enhance the competitiveness of the US semiconductor industry by facilitating coordination between the Pentagon and the industry to enhance production and collaboration.</p>\n<p>(Shortform note: While SEMATECH did achieve success by fostering collaboration among its member companies, <a href=\"https://www.nationaldefensemagazine.org/articles/2022/6/2/with-chips-down-sematech-gets-second-look\" target=\"_blank\">by the mid-2000s, the semiconductor landscape had evolved significantly</a>. New challenges emerged, including the globalization of the semiconductor supply chain and the rise of semiconductor manufacturing in Asia. SEMATECH’s funding model also faced challenges. It relied on contributions from member companies and government support, but sustaining funding levels became increasingly difficult. Eventually, the consortium was absorbed by the State University of New York Polytechnic Institute in 2015.)</p>\n<h5 id=\"the-japanese-begin-to-falter\">The Japanese Begin to Falter</h5>\n<p>By the 1990s, writes Miller, <strong>the Japanese position in the semiconductor industry began to decline.</strong></p>\n<p>In addition to the new American technology and improved public-private coordination described above, economic events in Japan contributed to the nation’s relative decline in the semiconductor space. During the 1990s, the Japanese economy took a downturn, while the United States emerged as the dominant economic force of the decade.</p>\n<p>Miller attributes this decline in Japan's economic prowess to several factors, including an excess of cheap capital and overinvestment. With such easy access to capital, Japanese firms found themselves less compelled to compete on the basis of quality. It made more sense for many Japanese companies to churn out commodified and generic chips—which were already being produced more affordably and effectively by South Korean companies like Samsung and American firms like Micron. While profitable for a time, this complacency and loss of competitive edge left the Japanese chip industry in a weak position to adapt to coming technological changes.</p>\n<blockquote><p><strong>Japan’s Lost Decade</strong></p>\n<p>The period of relative Japanese economic decline Miller describes is known to economic historians as “Japan’s Lost Decade”: <a href=\"https://www.investopedia.com/articles/economics/08/japan-1990s-credit-crunch-liquidity-trap.asp#:~:text=Japan's%20%22Lost%20Decade%22%20was%20a,down%20the%20real%20estate%20market.\" target=\"_blank\">a period of economic stagnation and financial crisis that extended from the early 1990s into the 2000s</a>. The era was marked by a number of economic events and trends:</p>\n<ul>\n<li><p><strong>Asset price bubble</strong>: The roots of Japan's economic troubles can be traced back to the late 1980s when the country experienced an asset price bubble, particularly in the real estate and stock markets. Speculation drove up property and equity prices to unsustainable levels—until the bubble burst, triggering a financial crisis.</p>\n</li>\n<li><p><strong>Credit crunch</strong>: The financial crisis led to a credit crunch, where banks became reluctant to lend, and businesses and consumers faced difficulties accessing credit.</p>\n</li>\n<li><p><strong>Deflation</strong>: One of the defining features of Japan's Lost Decade was deflation, a persistent decrease in the general price level. Falling prices discouraged consumer spending and business investment, as people expected goods and assets to become cheaper in the future.</p>\n</li>\n</ul>\n</blockquote>\n<h3 id=\"part-3-the-rise-of-tsmc-and-the-foundry-model\">Part 3: The Rise of TSMC and the Foundry Model</h3>\n<p>Up to now, we’ve explored the rise of the semiconductor industry in the United States and the robust challenge to the American industry from the Japanese that began in the 1960s. In this section, we’ll turn our attention to the rise of Taiwan Semiconductor Manufacturing Company (TSMC) and the Taiwanese chip industry beginning in the 1980s, the resulting shift to the foundry model for chip manufacturing, the corresponding shift to the “fabless” model for chip design, and the decentralization and globalization of the chip supply chain.</p>\n<h4 id=\"morris-chang-founds-tsmc\">Morris Chang Founds TSMC</h4>\n<p>In 1985, Chinese-born entrepreneur Morris Chang left Texas Instruments (TI) after being passed over for the CEO role. Spurned by the company to which he’d devoted his career, Miller writes that Chang decided to seize a different opportunity—<strong>to make Taiwan the new epicenter of the chip industry and place himself at its head</strong>. In 1987, he founded TSMC, a public-private partnership that would become the dominant global player in chip manufacturing.</p>\n<p>Fortunately for Chang’s ambitions, writes Miller, the Taiwanese government was eager to develop a domestic semiconductor industry to reduce its reliance on foreign technology and create a strategic economic advantage. Thus, they were enthusiastic about providing Chang’s initiative with significant support through financial incentives, access to resources, regulatory assistance, and tax incentives. Indeed, this backing was crucial for establishing the necessary infrastructure and acquiring advanced manufacturing equipment for large-scale chip production.</p>\n<blockquote><p><strong>TSMC Aims to Maintain Dominance by Globalizing Production</strong></p>\n<p>As its domestic semiconductor industry has grown, TSMC has maintained a strong desire to safeguard its competitive edge. One way of doing this is by investing in chip facilities in the US, which would help the company diversify its production locations and enhance supply chain resilience.</p>\n<p><a href=\"https://thediplomat.com/2023/01/tsmcs-us-investments-spark-political-controversy-in-taiwan/\" target=\"_blank\">This strategy has been a flashpoint of controversy in Taiwan</a>. In 2022, the state-backed firm announced plans to invest $40 billion in a new advanced chip manufacturing facility in Arizona. However, Taiwan’s economy and national security are closely tied to the semiconductor sector, and TSMC's investments in the US have sparked concerns in Taiwan about technology and intellectual property leakage to the US and possible future dependency on US facilities.</p>\n</blockquote>\n<h4 id=\"the-foundry-model\">The Foundry Model</h4>\n<p>Miller observes that <strong>what truly set TSMC apart from other chip companies was its pioneering role in spearheading what’s known as the foundry model</strong>.</p>\n<p>Instead of designing their own chips, TSMC only manufactured chips designed by <em>other </em>companies—they were a manufacturing workshop. This offered distinct business advantages for TSMC through economies of scale and the opportunity to hone its production capabilities.</p>\n<h5 id=\"the-advantage-of-economies-of-scale\">The Advantage of Economies of Scale</h5>\n<p>Miller writes that <strong>by mass producing chips for multiple customers, TSMC could achieve cost efficiencies through economies of scale</strong>. A small chip manufacturer faces relatively high costs to manufacture each chip—they have to set up and run their manufacturing equipment, pay for labor, and cover other fixed costs, with those costs spread out over a small number of chips. Thus, if the small manufacturer spends $100,000 on those startup costs but produces only 1,000 chips, their cost-per-chip is $1,000. But a large-scale mass producer like TSMC might produce 10 <em>million </em>chips, dropping their fixed costs down to $0.01 per chip.</p>\n<blockquote><p><strong>The Limits of Economies of Scale</strong></p>\n<p>Although economies of scale do offer significant advantages for large enterprises, there are limitations. As companies increase their production levels, <a href=\"https://www.indeed.com/career-advice/career-development/scale-vs-scope\" target=\"_blank\">there is a point at which cost savings from economies of scale may no longer be achievable</a>. Beyond a certain production threshold, cost reduction may not continue.</p>\n<p>For example, a company that manufactures smartphones will benefit from increasing its production volume as it negotiates better deals with suppliers, optimizes its manufacturing processes, and reduces per-unit production costs. However, the company is eventually operating at maximum capacity, and any <em>further </em>increase in production would require significant <em>new </em>investments in facilities and equipment.</p>\n</blockquote>\n<h5 id=\"tsmcs-production-edge\">TSMC’s Production Edge</h5>\n<p>Moreover, <strong>by focusing relentlessly on production, TSMC was able to optimize its manufacturing processes</strong>, invest in new research and development, and purchase cutting-edge lithography equipment at scale by buying in bulk. All of this made its manufacturing processes more efficient—leading to faster production times and ever-lower labor costs per chip.</p>\n<p>(Shortform note: Beyond bulk buying, TSMC’s scale affords it other advantages that contribute to its production edge. In particular, <a href=\"https://www.ft.com/content/05206915-fd73-4a3a-92a5-6760ce965bd9\" target=\"_blank\">TSMC's ability to stockpile critical materials</a>, such as silicon wafers, during periods of high demand or shortages, enhances its supply chain resilience. This helps ensure a steady production flow even when the company is faced with external supply disruptions.)</p>\n<h4 id=\"tsmcs-dominance\">TSMC’s Dominance</h4>\n<p>By achieving scale like this, TSMC was able to produce chips at a price that its competitors couldn’t beat.</p>\n<p><strong>TSMC secured a hammerlock on the manufacturing process for key electronics like iPhones and other smartphones</strong>—achieving this position after Intel famously turned down Steve Jobs for the iPhone contract, not seeing the potential of the smartphone market. With major customers like Apple, TSMC held 50% of the global chip foundry market by 2015.</p>\n<p>(Shortform note: In order to maintain its globally dominant position in the chip industry, TSMC has had to strategically expand production into other regions. However, <a href=\"https://www.nytimes.com/2023/08/04/technology/tsmc-mark-liu.html\" target=\"_blank\">TSMC is committed to preserving its Taiwanese identity</a>. The company's origins and headquarters are in Taiwan, where it has a significant presence and continues to play a vital role in the country's economy. The company's expansion into other countries, such as the United States and Japan, is driven by the need to diversify its production and reduce geopolitical risks but not to sever its ties with Taiwan. The company's presence in Taiwan remains integral to its identity and the broader semiconductor landscape, despite its strategic expansion to other regions.)</p>\n<h4 id=\"the-fabless-model\">The Fabless Model</h4>\n<p>Miller writes that<strong> TSMC’s success also heralded a revolution in chip design, as the chip design industry shifted toward what’s become known as the “fabless” model</strong>—in which the company focuses only on chip design rather than design <em>and </em>manufacturing.</p>\n<p>TSMC and other chip manufacturers enabled chip designers to outsource their manufacturing operations to companies engaged solely in manufacturing. By thus freeing themselves from the high startup costs of chip manufacturing, these designers can achieve the same optimization and efficiency in chip <em>design </em>that TSMC achieved in chip <em>manufacturing</em>.</p>\n<p>(Shortform note: Despite the advantages of the fabless model, <a href=\"https://www.eetimes.com/fabless-start-up-companies-face-myriad-operational-challenges/\" target=\"_blank\">fabless semiconductor startup companies do face limitations and operational challenges</a>. By relying on third-party foundries for chip production, fabless companies can face increased production costs and capital constraints, making it challenging for them to compete with integrated semiconductor firms. In addition, depending on external foundries for chip manufacturing introduces supply chain risks, as any disruptions or capacity constraints at the foundries can impact the production and delivery of their chips. Finally, limited control over the manufacturing process can affect the quality and consistency of fabless companies’ chips.)</p>\n<h5 id=\"the-decentralization-and-globalization-of-the-chip-supply-chain\">The Decentralization and Globalization of the Chip Supply Chain</h5>\n<p>In the 2000s and 2010s, more designers began to shift to the “fabless” model, in which these companies no longer needed the fabrication equipment to manufacture their chips. Miller writes that this shift was a major factor in the globalization and decentralization of the chip supply chain—with the United States holding a smaller share of chip manufacturing as that side of the industry moved to Taiwan and other East Asian countries.</p>\n<p>(Shortform note: Recognizing the declining role of the US in chip manufacturing, <a href=\"https://www.cnn.com/2022/10/18/tech/us-chip-manufacturing-semiconductors/index.html\" target=\"_blank\">American policymakers have sought to strengthen the country’s position in the global chip supply chain</a>. Specifically, the US government has allocated significant funding to bolster domestic chip manufacturing. This includes the 2022 CHIPS Act, which provides $52 billion in incentives for semiconductor manufacturing and research. The aim is to reduce dependence on foreign chip production and reduce vulnerabilities in the supply chain, especially in times of global crises.)</p>\n<h3 id=\"part-4-the-emergence-of-china\">Part 4: The Emergence of China</h3>\n<p>So far, we’ve explored the origins of the chip industry with a handful of American entrepreneurs after World War II, the emergence of the Japanese semiconductor industry in the 1960s, the rise of TSMC and the foundry and fabless models, and the globalization of chip manufacturing and design.</p>\n<p>In this section, we’ll turn our attention to the increasingly prominent role of the People’s Republic of China in the global chip game. Specifically, we’ll look at China’s early attempts to establish an onshore chip industry; the recognition by Chinese leadership of the importance of semiconductors for the nation’s economic, military, and national security future; and the geopolitical concerns and challenges that China’s emergence as a major player in the chip industry has for the US and its allies.</p>\n<h4 id=\"chinas-semiconductor-ambitions\">China’s Semiconductor Ambitions</h4>\n<p>According to Miller, <strong>in the late 20th and early 21st centuries China embarked on a significant journey to emerge as a global player in the semiconductor industry. </strong></p>\n<p>The Chinese government had a vision of making mainland China a global semiconductor powerhouse. They believed China’s low labor and manufacturing costs could lure semiconductor investment to the country, play a key role in the nation’s recovery from the radicalism and chaos of the Maoist era, and be the stepping stone for China to finally play a central role on the global stage.</p>\n<blockquote><p><strong>The Chinese Economic Reforms</strong></p>\n<p>To more fully understand China’s push to develop a domestic semiconductor industry, it’s worth exploring the historical context in which it’s taken place.</p>\n<p><a href=\"https://www.britannica.com/place/China/Educational-and-cultural-policy-changes\" target=\"_blank\">China initiated a series of economic reforms in the late 20th century, notably under the leadership of Deng Xiaoping</a>. These reforms began in the late 1970s and continued through the 1980s. In 1980, China designated Shenzhen as its first Special Economic Zone. These zones were created to attract foreign investment by offering favorable policies and fewer restrictions to foreign companies. At the same time, China embarked on a path of trade liberalization, reducing trade barriers and making it more enticing for foreign companies to invest. The country also made substantial investments in education and workforce development throughout the late 20th century, aiming to cultivate a skilled labor force. These efforts continued through the 1990s and beyond.</p>\n<p>As a result of these reforms and the influx of foreign investment, China experienced rapid economic growth, becoming one of the world's largest economies. This growth trajectory began in the late 20th century and continued into the 21st century.</p>\n</blockquote>\n<h4 id=\"chinas-early-challenges\">China’s Early Challenges</h4>\n<p>However, observes Miller, China’s early ventures faced formidable challenges. Although Chinese cities like Zhengzhou and Dongguan emerged as hubs for chip and smartphone assembly, they were concentrated on the lower end of the value chain—providing largely unskilled labor to assemble iPhones and other devices for foreign companies like Taiwanese giants Foxconn and Wistron.</p>\n<p>Despite the success of gaining a foothold in the low-value end of the chip value chain, China's semiconductor market share still trailed that of geopolitical rivals such as the US, South Korea, Japan, and, most significantly, Taiwan.</p>\n<blockquote><p><strong>The Opportunities and Challenges of China’s Unskilled Labor Pool</strong></p>\n<p>Miller notes that the main resource China could contribute to the global economy at this time was its vast pool of unskilled labor—indeed, this once provided a significant advantage for China’s manufacturing industries, allowing China to become the \"world's factory\" by offering low-cost labor for various production processes. As China's economy evolved and technology advanced, <a href=\"https://sccei.fsi.stanford.edu/china-briefs/invisible-china-hundreds-millions-rural-underemployed-may-slow-chinas-growth#:~:text=Construction%20jobs%20have%20tapered%20off,unemployable%20as%20the%20economy%20upgrades.\" target=\"_blank\">there was a shift toward a more skilled and educated workforce</a>. The country invested heavily in education and vocational training to develop a workforce capable of handling advanced manufacturing and technology-related jobs.</p>\n<p>However, many industries that relied on low-skilled labor faced rising labor costs as workers demanded higher wages and better working conditions. The shift away from unskilled labor impacted industries like textiles, apparel, and simple assembly, which had traditionally thrived on low-wage workers. Some of these industries began relocating to countries with lower labor costs.</p>\n</blockquote>\n<h4 id=\"the-push-for-geopolitical-dominance\">The Push for Geopolitical Dominance</h4>\n<p>Miller notes that the Chinese leadership—notably General Secretary of the Chinese Communist Party Xi Jinping—recognized that <strong>China's economic, military, and national security future depended on developing a homegrown, high-value semiconductor industry</strong>. They needed to move beyond assembling devices for global tech giants like Apple and Samsung and begin leading in chip design and manufacture. Beyond the economic benefits, there were also national security and geopolitical strategic concerns behind building a domestic chip industry—key technologies underpinning China's massive surveillance state heavily relied on inputs of foreign-made chips. If those supplies were suddenly cut off, China would be at risk.</p>\n<p>In the 2010s, China began pouring tens of billions into its information technology sector, pushing both private investors and state-owned enterprises to invest and shift the center of technological innovation eastward.</p>\n<p>(Shortform note: <a href=\"https://foreignpolicy.com/2023/04/03/chips-biden-xi-china-sanctions-semiconductors/\" target=\"_blank\">China's efforts have faced resistance from various quarters, including the United States and its allies</a>. The United States has imposed sanctions on Chinese semiconductor companies and restricted their access to advanced technology, particularly American-made equipment and software. These sanctions aim to curb China's rapid advancement in semiconductor manufacturing. In addition, the US is working to strengthen alliances with like-minded countries to collectively counter China's semiconductor ambitions. This includes efforts to coordinate export controls and technology restrictions.)</p>\n<h5 id=\"china-targets-western-firms\">China Targets Western Firms</h5>\n<p>Miller writes that, on the heels of China’s infusion of state investment into its domestic chip industry, <strong>Western firms, eager to get a foothold in the massive and lucrative emerging Chinese market, began doing more and more business with China</strong>. Companies like AMD, IBM, Qualcomm, SoftBank, and Arm began licensing core technology to Chinese companies or entering joint ventures with Chinese government-backed enterprises. While this arrangement has been profitable for these companies, critics raised the alarm that these companies’ eagerness to tap into China gave the Chinese authorities leverage over them. The concern is that the Chinese could exploit this leverage to gain access to important technology with national security implications for the US and its allies.</p>\n<p>Meanwhile, Chinese interests also sought to buy stakes in US semiconductor companies, further complicating the landscape. Chinese mega-firms like Huawei rose to prominence, grabbing significant global market shares through replication and, at times, alleged technology theft, while also leveraging their position as major customers of massive suppliers like TSMC.</p>\n<blockquote><p><strong>TikTok and the Global Concern About Chinese Espionage</strong></p>\n<p>Concern about Chinese technology intrusion extends beyond the semiconductor industry. The White House, Congress, US armed forces, and <a href=\"https://www.cnn.com/2023/01/16/tech/tiktok-state-restrictions/index.html\" target=\"_blank\">more than half of US states</a> have <a href=\"https://shortform.com/app/article/the-challenges-and-perils-of-a-us-tiktok-ban-shortform-explainers-shortform\" target=\"_blank\">banned the Chinese-owned social media app, TikTok, from government-issued devices</a>. There’s broad concern that TikTok’s parent company, ByteDance, could share Americans’ personal data with the Chinese government, or push Chinese propaganda and misinformation.</p>\n<p>TikTok has strenuously denied sharing user data with the authoritarian government, and in fact, studies from 2023 and 2021 found TikTok’s data collection practices to be comparable to—and no more of a threat than—those of US social media apps.</p>\n<p>Many experts argue that a <em>total </em>TikTok ban is problematic. Civil liberties groups say a nationwide ban likely <a href=\"https://www.usatoday.com/story/opinion/2023/03/13/tiktok-ban-congress-threaten-first-amendment-rights/11435946002/\" target=\"_blank\">could create a dangerous precedent</a> for the US government to dictate how Americans communicate—<a href=\"https://hls.harvard.edu/today/will-the-us-ban-tiktok/\" target=\"_blank\">negatively impacting millions of influencers</a> and other US citizens whose identity, self-expression, communications, and financial health are tied to the app.</p>\n</blockquote>\n<h4 id=\"the-global-chip-competition\">The Global Chip Competition</h4>\n<p>Miller writes that <strong>these developments showcase the complex interplay of economics, technology, and geopolitics in the semiconductor industry</strong>. Computing power, the ability to produce chips and complex systems, and the capacity to transmit data faster and more accurately are the modern keys to geopolitical influence and military prowess.</p>\n<p>The semiconductor industry had become a focal point in the US-China rivalry, and the stakes were higher than ever. The world was beginning to understand that the struggle for dominance in this industry was not merely an economic or industrial issue—but a matter of national security and global power.</p>\n<blockquote><p><strong>Is the US Winning the Chip War?</strong></p>\n<p>Some experts argue that in the global semiconductor competition between the United States and China, <a href=\"https://www.bbc.com/news/world-asia-pacific-64143602\" target=\"_blank\">the US is currently leading</a>. Several key factors contribute to the US's advantage in this race:</p>\n<p><strong>Technological innovation</strong>: The US remains a hub for cutting-edge semiconductor design and development. Companies like Intel, NVIDIA, and Qualcomm continue to drive innovation in chip technology, ensuring the US maintains a significant lead in creating advanced semiconductor solutions.</p>\n<p><strong>Export controls</strong>: The US government has implemented stringent export controls that limit China's access to advanced semiconductor technology. These controls restrict the sale of chips, chip-making equipment, and software containing US tech to China, significantly hampering China's semiconductor progress.</p>\n<p><strong>Domestic investment</strong>: The US has initiated programs like the CHIPS Act, offering substantial grants and subsidies to companies involved in semiconductor manufacturing within the US. Meanwhile, major players like TSMC and Micron are investing billions in new facilities in the US, leveling the playing field against Asian competition.</p>\n</blockquote>\n"
  },
  {
    "cover_image": "https://media.shortform.com/covers/png/the-design-of-everyday-things-cover.png",
    "title": "The Design of Everyday Things",
    "author": "Don Norman",
    "tags": [
      "Arts/Design",
      "Technology"
    ],
    "url_slug": "the-design-of-everyday-things",
    "html": "<p>Every man-made object, environment, or program in our world is designed. From doorknobs to smartphone apps, design pervades our lives to the point that it often becomes completely invisible. When we struggle with one of these designs, we assume that our difficulties are our own fault, or that we’re just not smart enough to figure it out. But that blame is misplaced. More often than not, the true culprit in cases of “human error” is actually bad design.</p>\n<p>In <em>The Design of Everyday Things</em> (originally released in 1988 under the title <em>The Psychology of Everyday Things </em>and revised in 2013), cognitive psychologist and engineer Don Norman explores the ways people understand and interact with the physical environment (this is sometimes referred to as “user experience”). In doing so, he makes all of us smarter consumers and helps designers create products that work <em>with</em> users, rather than against them.</p>\n<h3 id=\"interacting-with-objects\">Interacting With Objects</h3>\n<p>At its core, design is any human influence on the physical world. This applies to everything from ancient architectural marvels to the layout of clothes in your closet.</p>\n<p>When we interact with design, we’re guided by the principles of <em>discoverability</em> and <em>understanding</em>. <strong>Discoverability</strong> refers to whether a user can figure out what an object is and how to use it without considerable effort. Discoverability answers the question, “How do I use this thing?” <strong>Understanding</strong>, in this context, refers to the user’s ability to make meaning out of the discoverable features of the object. Understanding answers the questions, “What is this, and why do I want to use it in the first place?”</p>\n<p>Focusing on these factors is a hallmark of human-centered design, which is a design philosophy that flips the traditional design process on its head by <strong>focusing on human needs and behaviors <em>first</em> </strong>and designing products to fit those needs, rather than designing a product and hoping that users figure out how to use it.</p>\n<h4 id=\"how-do-we-know-how-to-use-an-object\">How Do We Know How to Use an Object?</h4>\n<p>To design for human needs, we need to understand how people interact with design. There are six design principles that influence how we interact with an object: affordances, signifiers, mapping, feedback, models, and the system image.</p>\n<p><strong>Affordances</strong> are the finite number of ways in which a user can possibly interact with a given object. They answer the question, “What is this thing <em>for</em>?” For example, chairs typically have a flat surface, which we intuitively recognize as an indicator of support. In other words, the look of a chair suggests that it is <em>for </em>sitting on.</p>\n<p><strong>Signifiers </strong>are signals that draw the user’s attention to an affordance they may not have intuitively discovered, like a “click here” button on a website or a “push” sign on a door. <strong>For designers, signifiers are more important than affordances</strong>: The most sophisticated technology is pretty useless if a user can’t find the “on” button.</p>\n<p><strong>Mapping</strong> uses the position of two objects to communicate the relationship between them. For example, if you see a row of three lights and a panel of three switches, natural mapping would mean the position of the switch corresponds to the position of the light it controls. Mapping is not universal since culture can influence how we think about direction and spatial relationships.</p>\n<p><strong>Feedback </strong>is a sensory signal that alerts the user that what they’re doing to an object is having some effect. Feedback can tell us when something is working as expected, but more importantly, when it’s <em>not</em> working how we want. In a car, a dashboard alert light or the sound of squeaking brakes are both sources of feedback that let us know something is wrong.</p>\n<p><strong>Models</strong> (also called conceptual models or mental models) are mental images of an object and how it works based on affordances, signifiers, mapping, and feedback. Mental models stem from the universal instinct to organize information into cohesive stories. But these stories are not always accurate, and false mental models of a design can cause confusion.</p>\n<ul>\n<li>For example, many people have an inaccurate model of their home thermostat. They assume the thermostat controls a valve that opens a certain amount based on the setting, and that setting it higher will warm the room faster. In reality, most thermostats are a simple on/off switch, so setting a higher temperature has no effect on how fast the room warms up.</li>\n</ul>\n<p><strong>The System Image </strong>is the sum total of the information we have about an object, including both its physical properties and information from user manuals, product websites, or past experience. The system image is the only way designers can communicate their model of how something works to the user.</p>\n<h3 id=\"cognition-emotion-and-behavior\">Cognition, Emotion, and Behavior</h3>\n<p>The way we think clearly influences how we interact with objects, but designers often underestimate the role of psychology in user interaction.</p>\n<h4 id=\"the-seven-stages-of-action\">The Seven Stages of Action</h4>\n<p>When we interact with an object, we face two “gulfs” of understanding: <strong>the Gulf of Execution</strong>, (figuring out what an object does and how to use it) and <strong>the Gulf of Evaluation</strong> (evaluating results after using the object). To cross these gulfs, we use a seven stage action cycle. This action cycle happens unconsciously unless we’re interacting with an unfamiliar or confusing object. Each stage answers a particular question.</p>\n<ol>\n<li>Goal: What result do I want to achieve?</li>\n<li>Plan: What options do I have for achieving my goal?</li>\n<li>Specify: Which of these options will I choose?</li>\n<li>Perform: How do I execute my plan?</li>\n<li>Perceive: What happened when I did that?</li>\n<li>Interpret: What does that result mean?</li>\n<li>Compare: Did I reach my goal?</li>\n</ol>\n<p>Let’s use grocery shopping as an example to see the seven steps in action. In that case, they may look something like this:</p>\n<ol>\n<li>Goal: I need to go grocery shopping. </li>\n<li>Plan: Should I drive to the store or take the bus?</li>\n<li>Specify: I think I’ll drive. </li>\n<li>Perform: I’ll follow the usual route to the store instead of a new one.</li>\n<li>Perceive: Everything went smoothly and I’ve parked at the store.</li>\n<li>Interpret: This means I can now go inside and shop.</li>\n<li>Compare: I’ve met my goal of going grocery shopping!</li>\n</ol>\n<p>This cycle will play out multiple times for any given action because most behaviors have both an overall goal (like “go grocery shopping”) composed of several subgoals (“start the car”). Determining the overall goal is important because it gives designers a better idea of what users really want. To do this, we use <strong><em>root cause analysis</em>, or continually asking “why?” about a behavior until there is no further answer. </strong>The root cause of a behavior might be internal goal-driven (studying for a test) or external event-driven (putting in earplugs in a noisy environment).</p>\n<h4 id=\"conscious-vs-subconscious-processing\">Conscious vs. Subconscious Processing</h4>\n<p>People react to technology in their lives through thoughts and emotions, and good design can capitalize on those reactions. To do that well, designers need an accurate working model of how the brain processes information.</p>\n<p>There are two types of cognitive processing: conscious and subconscious. Conscious processing is deliberate: It’s where we compare options, predict possible outcomes, and come up with new ideas. Subconscious processing is automatic and works in generalizations. It’s the home of pattern recognition and snap judgments.</p>\n<p>We can think of these differences in terms of three levels of processing: visceral, behavioral, and reflective.</p>\n<ul>\n<li><strong>The visceral level </strong>is subconscious and involves our most primitive reflexes, like startling at a loud noise or flinching when something flies towards us unexpectedly. Visceral reactions can have a powerful influence on how users respond to an object. An otherwise well-designed product can fail if it provokes a negative visceral response in the user (like with a sudden, blaring alarm, or an unpleasant odor.)</li>\n<li><strong>The behavioral level</strong> is the home of the subconscious process of turning thought into action. It fills the gap between intention (like speaking) and action (moving your lips, tongue, and jaw in specific ways). This level is important in design because it functions based on expectations—if you flip a light switch, you subconsciously expect a light to turn on. If it doesn’t, your ability to process the action is interrupted by bad design.</li>\n<li><strong>The reflective level</strong> is where conscious processing happens. This is where we interpret information from the other levels and use those conclusions to make decisions. If a product “rubs us the wrong way” on the visceral level, the reflective level might evaluate that information and consciously decide not to use that product again.<ul>\n<li>For example, if an alarm clock keeps perfect time and is easy to use, but the alarm sound itself is so loud and jarring that you wake up each morning thinking the house is on fire, the memory of that visceral response might make you view the interaction negatively and avoid that specific clock (or brand) in the future. </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"memory\">Memory</h4>\n<p>Memory also impacts our interactions with objects. There are two kinds of knowledge: “knowledge in the head” (memory) and “knowledge in the world,\" which is anything we don’t have to remember because it’s contained in the environment (like the letters printed on keyboard keys). Putting knowledge into the world frees up space in our memories and makes it easier to use an object.</p>\n<p><strong>The knowledge we keep in our heads is only as precise as the environment requires</strong>. Most people won’t notice if you change the silhouette on an American penny because we only need to remember the color and size to tell a penny apart from other coins. We’re more likely to notice changes to the portrait on an American dollar bill, because we’re used to relying on that image to help us tell bills apart (since they are identical in size, shape, and color).</p>\n<p>Memories can be stored either short- or long-term. Short-term memory is the automatic storage of recent information. We can store about five to seven items in short-term memory at a time, but if we lose focus, those memories quickly disappear. This is important for design: Any design that requires the user to remember something is likely to cause errors.</p>\n<p>Long-term memory isn’t limited by time or number of items, but memories are stored subjectively. Meaningful things are easy to remember; arbitrary things are not. To remember arbitrary things, we need to impose our own meaning through mnemonics or approximate mental models. Designers can make this easier for users by making arbitrary information map onto existing mental models (for example, think of the way Apple has kept the location of the power and volume buttons relatively the same with each new version of the iPhone.)</p>\n<h3 id=\"the-error-of-human-error\">The Error of “Human Error”</h3>\n<p>Industry professionals estimate that between 75 and 95 percent of industrial accidents are attributed to human error. This number is misleading, since what we think of as “human errors” are more likely outcomes of a system that has been unintentionally designed to <em>create </em>error, rather than prevent it.</p>\n<h4 id=\"detecting-errors\">Detecting Errors</h4>\n<p>Errors can be divided into “slips” (errors of <em>doing</em>) and “mistakes” (errors of <em>thinking</em>). Accidentally putting salt instead of sugar in your coffee is a slip—your thinking was correct, but the action went awry. Pressing the wrong button on a new remote control is a mistake—you carried out the action fine, but your thought about the button’s function was wrong.</p>\n<p><strong>Most everyday errors are slips, </strong>since they happen during the subconscious transition from thinking to doing. Slips happen more frequently to experts than beginners, since beginners are consciously thinking through each step of a task. On the other hand, mistakes are more likely to happen in brand new scenarios where we have no prior experience to pull from, or even familiar scenarios if we misread the situation.</p>\n<h4 id=\"causes-of-error\">Causes of Error</h4>\n<p>One major cause of error is that our technology is engineered for “perfect” humans who never lose focus, get tired, forget information, or get interrupted. Unfortunately, these humans don’t exist. Interruptions in particular are a major source of error, especially in high-risk environments like medicine and aviation.</p>\n<p>Social and economic pressures also cause error. The larger the system, the more expensive it is to shut down to investigate and fix errors. As a result, people overlook errors and make questionable decisions to save time and money. If conditions line up in a certain way, what starts as a small error can escalate into disastrous consequences.</p>\n<ul>\n<li>Social and economic pressures played a critical role in the Tenerife airport disaster, when a plane taking off before receiving clearance crashed into another plane taxiing down the runway at the wrong time. The first plane had already been delayed, and the captain decided to take off early to get ahead of a heavy fog rolling in, ignoring the objections of the first officer. The crew of the second plane questioned the unusual order from air traffic control to taxi on the runway, but obeyed anyway. Social hierarchy and economic pressure led <em>both</em> crews to make critical mistakes, ultimately costing 583 lives.</li>\n</ul>\n<h4 id=\"preventing-errors\">Preventing Errors</h4>\n<p>Good design can minimize errors in many ways. One approach is <strong>resilience engineering</strong>, which focuses on building robust systems where error is expected and prepared for in advance. There are three main tenets of resilience engineering.</p>\n<ol>\n<li>Consider <em>all </em>the systems involved in product development (including social systems).</li>\n<li>Test under real-life conditions, even if it means shutting down parts of a system.</li>\n<li>Test continuously, not as a means to an end, since situations are always changing.</li>\n</ol>\n<h5 id=\"constraints\">Constraints</h5>\n<p>Designers can also use <strong>constraints</strong>, which limit the ways users can interact with an object. There are four main types of constraints: physical, cultural, semantic, and logical.</p>\n<p><strong>Physical constraints</strong> are physical qualities of an object that limit the ways it can interact with users or other objects. The shape and size of a key is a physical constraint that determines the types of locks the key can fit into. Childproof caps on medicine bottles are physical constraints that limit the type of users who can open the bottle.</p>\n<p><strong>Cultural constraints </strong>are the “rules” of society that help us understand how to interact with our environment. For example, when we see a traditional doorknob, we expect that whatever surface it’s attached to is a door that can be opened. This isn’t caused by the design of the doorknob, but by the cultural <strong>convention</strong> that says “knobs open doors.\"</p>\n<p>When these agreements about how things are done are codified into law or official literature, they become <strong>standards</strong>. We rely on standards when design alone isn’t enough to make sure everyone knows the “rules” of a situation (for example, the layout of numbers on an analog clock is standardized so that we can read any clock, anywhere in the world).</p>\n<p>Although they’re less common, semantic and logical constraints are still important. <strong>Semantic constraints</strong> dictate whether information is meaningful. This is why we can ignore streetlights while driving, but still notice brake lights—we’ve assigned meaning to brake lights (“stop!”), so we know to pay attention and react.</p>\n<p>Logical constraints make use of fundamental logic (like process of elimination) to guide behavior. For example, if you take apart the plumbing beneath a sink drain to fix a leak, then discover an extra part leftover after you’ve reassembled the pipes, you know you’ve done something wrong because, logically, all the parts that came out should have gone back in.</p>\n<h3 id=\"the-design-thinking-process\">The Design Thinking Process</h3>\n<p><strong>“Design thinking” </strong>is the process of examining a situation to discover the root problem, exploring possible solutions to that problem, testing those solutions, and making improvements based on those tests. This process is <em>iterative</em>, which means it is repeated as many times as necessary, each time with slight improvements based on previous iterations.</p>\n<p>Design thinking involves two tasks: finding the right problem and finding the right solution. Designers are often hired to solve symptoms, but good designers dig deeper to find the underlying problem before coming up with solutions. To do this, designers run through four stages: <strong>observation, idea generation, prototyping, and testing</strong>. This process is repeated as many times as necessary to develop the final product.</p>\n<p>The <strong>observation </strong>phase involves gathering information on the people who will use the new design. This is different from market research:<strong> </strong>Designers want to know what people need and how they might use certain products, while marketers want to know which groups of people are most likely to buy the product.</p>\n<p>After observation comes the <strong>idea generation</strong> phase, where designers brainstorm solutions to the problem. The goal is to generate as many ideas as possible without censoring “silly” ideas, since they might spark valuable discussion. Designers will then create <strong>prototypes </strong>of the most promising ideas using things like sketches and cardboard models.</p>\n<p>Once the prototype is refined, the <strong>testing </strong>phase begins, where members of the target user group are asked to try out the prototype and give their feedback. Designers then repeat the entire process based on the feedback from the first round of testing. The iterative design thinking process emphasizes testing in small batches with refinement in between rather than waiting until the final product and testing with a much larger group.</p>\n<h3 id=\"design-thinking-in-the-real-world\">Design Thinking in the Real World</h3>\n<p>In reality, the design process often doesn’t live up to the above ideal. Business pressures are the primary culprit here, since a well-designed product will still fail if it’s over budget and past deadlines. Product development team dynamics are also a challenge. The best teams are multidisciplinary, combining unique knowledge from different fields. However, each team member usually thinks their discipline is the most important.</p>\n<p>Diversity among users can also impact design. For users with disabilities, designers can turn to a <strong>universal design</strong> approach. Universal design creates products that are usable by the widest range of people by designing for the highest need, not the average need. Adopting a universal design approach changes how designers choose the types of people and environments to observe as well as the features they focus on most in the prototyping and testing phases.</p>\n<p>This approach is “universal” because if a product, environment, or service is designed with disability access in mind, it will typically also be usable for those without disabilities. For example, curb cuts were originally designed for wheelchair users but are also enormously helpful for anyone pushing a stroller or lugging a suitcase.</p>\n<h4 id=\"technological-innovation\">Technological Innovation</h4>\n<p>Economic pressures drive innovation. This can take the form of <strong>“featuritis,\"</strong> or the tendency to add more and more features to a product to keep up with competitors. These features ultimately degrade the design quality of the original product. Rather than winning over customers with new features, it’s better to do one thing better than anyone else on the market.</p>\n<p>Real quality innovation can be either radical or incremental. Radical innovation involves high-risk, game-changing ideas while incremental innovation makes small improvements to existing products over time. The invention of the automobile was radical—all the small improvements that led to cars as we know them today happened incrementally.</p>\n<h3 id=\"the-future-of-technology\">The Future of Technology</h3>\n<p>Rapid technological innovation raises questions about the future of user experience. The way we interact with objects around us will certainly change in response to new technologies, and cultural conventions will change to reflect that. But human <em>needs</em> will remain the same. For example, the keyboard has evolved from mechanical typewriters to computer keyboards to touchscreen versions, but the need to record written information has stayed the same. <strong>In other words, human needs won’t change, but the way they’re satisfied will.</strong></p>\n<p>Some people fear that the rise of smart technology is making humans <em>less</em> intelligent because we can delegate even the most basic tasks to machines—and if those machines fail, we are totally helpless. It’s true that some traditional skills are becoming obsolete thanks to new technology, but that process ultimately makes us <em>smarter</em>. The energy saved by not having to create a fire every time we need heat or light or rely on long division for simple calculations can be channeled into higher-level pursuits. <strong>Our intelligence hasn’t changed, only the tasks we apply it to. </strong>The key is in using technology to do the jobs technology can and <em>should</em> do.</p>\n<h4 id=\"increased-creation-and-consumption\">Increased Creation and Consumption</h4>\n<p>Technological innovation has made it easier than ever for anyone with a computer to create and publish new media. While amateur content creation has gotten easier, creating professional content has gotten harder and more expensive. The accessibility of smart technology levels the playing field, but makes it much harder to find quality, fact-checked content.</p>\n<p>For manufacturers, new technologies present a different challenge. The need to entice buyers is a fundamental part of business, because a product that doesn’t sell is a failure, no matter how well designed it is. <strong>But while services like healthcare and food distribution are self-sustaining (because there will always be a need for them), durable physical goods are not. </strong>If everyone who needs a particular product purchases one, there’s no one left to sell it to; If everyone already owns a smartphone, how do you convince them to buy the new and improved model?</p>\n<p>One way manufacturers get around this is through <em>planned obsolescence</em>, the practice of designing products that will break down after a certain amount of time and need to be replaced. This creates a cycle of consumption: buy something, use it until it breaks, throw it away, and buy another. While this cycle is good for business, the waste it generates is horrible for the environment. Thankfully, the combination of new technologies and a growing cultural awareness of sustainability issues is creating a new paradigm. The future of technology involves products designed with both the user and the environment in mind.</p>\n"
  },
  {
    "cover_image": "https://media.shortform.com/covers/png/the-innovators-dilemma-cover.png",
    "title": "The Innovator's Dilemma",
    "author": "Clayton M. Christensen",
    "tags": [
      "Business",
      "Entrepreneurship",
      "Technology"
    ],
    "url_slug": "the-innovator-s-dilemma",
    "html": "<p>Successful companies who pay attention to what their customers want and invest their resources in what generates the most profits usually continue to succeed. However, when a disruptive innovation shakes up an industry, these normally sound business practices can drive established market leaders to fail. This phenomenon is what Harvard Business School professor and leading expert on business innovation Clayton M. Christensen calls “the innovator’s dilemma.”</p>\n<p>In <em>The Innovator’s Dilemma,</em> Christensen first presents a theory to explain why disruptive innovations can cause business practices that normally bring success to bring failure instead, and then offers advice on how to adopt disruptive innovations successfully. Christensen published the first edition of The Innovator’s Dilemma in 1997, during the height of the era’s tech boom. The book was hailed as one of the best business books not only <a href=\"https://www.hbs.edu/news/releases/Pages/christensenmckinsey2011.aspx\" target=\"_blank\">of that year</a>, but <a href=\"https://www.economist.com/books-and-arts/2011/06/30/aiming-high\" target=\"_blank\">ever written</a>, and it was quickly adopted as a resource for CEOs and management leaders.</p>\n<p>In this guide, we’ll examine Christensen’s theory and advice, and we’ll contrast it with the perspectives of other experts on the subject, such as Geoffrey Moore, W. Chan Kim, and Renee Mauborgne. Moore is the author of <em><a href=\"https://www.shortform.com/app/book/crossing-the-chasm\" target=\"_blank\">Crossing the Chasm</a></em>, which addresses the same issue from the perspective of a start-up company <em>trying</em> to displace market leaders with a disruptive innovation. Kim and Mauborgne are the authors of <em><a href=\"https://www.shortform.com/app/book/blue-ocean-strategy\" target=\"_blank\">Blue Ocean Strategy</a></em>, in which they present a strategy for introducing disruptive innovations to reshape the market in ways that make your competitors irrelevant and give your company room to grow.</p>\n<h3 id=\"why-disruptive-innovations-are-disruptive-to-established-companies\">Why Disruptive Innovations Are Disruptive to Established Companies</h3>\n<p>To explore Christensen’s theory, we’ll first examine what a disruptive innovation is and the difference between <em>disruptive</em> innovations and <em>sustaining</em> innovations. Then we’ll discuss how these innovations disrupt markets and their characteristics that create problems for established companies.</p>\n<h4 id=\"disruptive-versus-sustaining-innovations\">Disruptive Versus Sustaining Innovations</h4>\n<p>Christensen defines a “<strong>disruptive innovation</strong>” as <strong>any new product that disrupts the mainstream market</strong>. For example, in the early 1900s mechanical tractors took over the draft-horse market. He also calls “disruptive innovation” a “discontinuous” innovation because it causes previous technologies or business practices to be discontinued.</p>\n<p>Christensen explains that typically, a disruptive innovation involves a drastically different approach to doing something, which, at first, doesn’t appeal to mainstream customers. Instead, it appeals only to a developing niche of customers with different needs. However, as the new product becomes more refined, its capabilities increase to the point where it offers greater value than traditional options in the mainstream market. At that point, he says, mainstream customers begin adopting it en masse, disrupting the market.</p>\n<p>Christensen contrasts disruptive innovations with “sustaining innovations,” which he defines as incremental improvements to a product that don’t alter the market. For example, an early tractor manufacturer might switch from using two-cylinder engines to four-cylinder engines. This makes their tractors more powerful and efficient, but it doesn’t change who buys the tractors or why they buy them. Sustaining innovations are also called “continuous” innovations.</p>\n<blockquote><p><strong>Comparing Definitions of Disruptive Innovation</strong></p>\n<p>Geoffrey Moore provides a slightly different definition of the difference between sustaining and disruptive innovations.</p>\n<p>According to Moore, a disruptive (or discontinuous) innovation is any new product that requires the customer to change how he uses a product in order to adopt it. For example, a farmer who sells his plow horses and buys a mechanical tractor now has to buy fuel for the tractor instead of feeding hay to his horses. Instead of having a ferrier periodically put new shoes on his horses, he has to have a technician perform maintenance on his tractor. He has to develop new skills and work with different suppliers to make use of the new technology.</p>\n<p>Similarly, according to Moore, a sustaining (or continuous) innovation is a product that is improved in some way but doesn’t require the customer to change in order to benefit from the improvement. For example, a farmer who upgrades from a two-cylinder tractor to a four-cylinder tractor can still buy fuel from the same supplier and have it serviced by the same technician.</p>\n<p>Thus, to Moore, a disruptive innovation is disruptive <em>to the customer’s life or routine</em>, while to Christensen, a disruptive innovation is <em>disruptive to the market</em>. In principle, these are fundamentally different definitions. Hypothetically, if John Deere introduced an autonomous tractor that required farmers to learn C++ programming, and farmers gradually adopted it without disrupting the market, then Moore would call it a disruptive innovation, while Christensen would call it a sustaining innovation.</p>\n</blockquote>\n<h4 id=\"how-disruptive-innovations-reshape-markets\">How Disruptive Innovations Reshape Markets</h4>\n<p>Christensen outlines the typical chronology by which disruptive innovations displace established companies:</p>\n<ol>\n<li>Someone invents a disruptive product, forms a start-up company, and begins producing it. </li>\n<li>Established companies evaluate the product and decide it wouldn’t be profitable in their current market, so they don’t pursue it.</li>\n<li>Through trial and error, the start-up finds an emerging market for their product. </li>\n<li>The start-up continues to refine their product. Eventually, its value (capabilities and reliability at its price point) surpasses traditional alternatives, and mainstream customers become interested in it.</li>\n<li>The disruptive product begins to dominate not only the emerging market that formed around it, but also the mainstream market, displacing traditional alternatives.</li>\n<li>Established companies realize that they are losing market share to the disruptive innovation, but by then it’s too late for them to defend their market share, even if they adopt the innovation themselves.</li>\n</ol>\n<blockquote><p><strong>Comparing Chronologies</strong></p>\n<p>Christensen presents his chronology of disruptive innovations from the standpoint of a business analyst and advisor to established companies. Moore presents a similar chronology from the standpoint of <a href=\"https://www.shortform.com/app/book/crossing-the-chasm/part-2-chapter-3\" target=\"_blank\">giving a start-up company a battle plan</a> for conquering the mainstream market. Let’s consider Moore’s chronology and compare it to Christensen’s:</p>\n<ol>\n<li><p>You come up with a revolutionary product. (This is the first step in disruptive innovation according to almost any expert.)</p>\n</li>\n<li><p>You sell your first few units to curious customers who try it out and corroborate the claim that you’ve created a novel product. These early enthusiasts probably won’t buy enough for you to make much money, but they help you refine the product and get the word out to a broader audience by word of mouth. In Christensen’s chronology, this is step three. His step two says that established companies first reject the new product—a step that Moore, in this chronology, doesn’t address.</p>\n</li>\n<li><p>You connect with a few ambitious customers who see your product’s future potential. They want to use your product for something specific and have the capital to implement it. You sign a lucrative contract with them to improve and customize your product to meet their needs. A few such contracts make up the bulk of what Moore calls the “early market.” Moore’s “early market” corresponds to Christensen’s “emerging market,” so this step closely corresponds to Christensen’s third step.</p>\n</li>\n<li><p>You select a single niche application for your product and refine it to provide indisputably better value in that application, allowing you to become the market leader in that niche. This corresponds to the first half of Christensen’s fourth step, though he doesn’t mention niche applications explicitly.</p>\n</li>\n<li><p>You continue to refine your product and expand into an adjacent niche. Repeat. As you become the market leader in more and more sectors, eventually you dominate the entire mainstream market. This corresponds to the second half of step four and step five of Christensen’s chronology. At this point, Moore’s mission is accomplished, so he doesn’t go on to discuss the fate of the former market leaders that you displaced, as Christensen does in his sixth step.</p>\n</li>\n</ol>\n</blockquote>\n<h4 id=\"problems-that-disruptive-innovations-cause-for-established-companies\">Problems That Disruptive Innovations Cause for Established Companies</h4>\n<p>Why don’t established companies just embrace the innovation at step two and reap its benefits, instead of falling behind until step six? Christensen explains that there are a number of reasons established companies rarely adopt disruptive innovations before it’s too late.</p>\n<h5 id=\"1-disruptive-innovations-get-started-in-small-markets\">1. Disruptive Innovations Get Started in Small Markets</h5>\n<p>As Christensen points out, it takes a big market to sustain a big company. However, when they first appear, disruptive innovations usually appeal only to a niche market that’s too small to be of interest to large, established companies. Thus, the companies that pursue disruptive innovations tend to be start-ups that are small enough to cater to small markets. If the market grows as the technology matures, the companies have a chance to grow with the market. However, by the time the market gets big enough for big companies to take notice, it may be too late for those larger companies to catch up.</p>\n<p>(Shortform note: Marketing experts Al Ries and Jack Trout explain that in any given market, <a href=\"https://www.shortform.com/app/book/positioning/1-page-summary#2-identify-your-desired-position\" target=\"_blank\">the first company</a> to assume a position of market leadership in the minds of customers usually remains the market leader. Thus, market forces tend to prevent late adopters from claiming much of the market share, even in cases where the disruptive innovation doesn’t involve a significant technological learning curve.)</p>\n<h5 id=\"2-disruptive-innovations-dont-appeal-to-existing-customers\">2. Disruptive Innovations Don’t Appeal to Existing Customers</h5>\n<p>Christensen explains that when a disruptive innovation first appears, an established company’s customers usually have no interest in it, because its performance characteristics are usually inferior to more mature technologies. Initially, the new innovation doesn’t compete for the same customers that buy the company’s current products. The few customers that it does attract have special interests that the new product uniquely addresses.</p>\n<blockquote><p><strong>The Technology Adoption Life Cycle</strong></p>\n<p>Christensen says that the market for disruptive innovations usually starts out as a small, niche market. Others have also studied this phenomenon and developed models to explain it.</p>\n<p>Probably the best-known model is the <a href=\"https://ageconsearch.umn.edu/record/17351/files/ar560111.pdf\" target=\"_blank\">“Diffusion of Innovations,”</a> which was originally developed by two agricultural scientists George Beal and Joe Bohlen. This model was generalized and <a href=\"https://www.simonandschuster.com/books/Diffusion-of-Innovations-5th-Edition/Everett-M-Rogers/9780743222099\" target=\"_blank\">popularized by Everet Rogers</a>, and then refined by Moore, who called it the <a href=\"https://www.shortform.com/app/book/crossing-the-chasm/part-1-chapter-1\" target=\"_blank\">“Technology Adoption Life Cycle,”</a> or TALC.</p>\n<p>As Moore presents it, the TALC predicts that the market for any disruptive innovation is made up of five groups of people, who become interested in the technology at different stages of its development:</p>\n<ul>\n<li><p><strong>“Techies” or “Innovators”</strong> love new technology for its own sake. They buy new products just to try them out and are happy to tinker with things in order to make them work, but they only represent about 2% of the population.</p>\n</li>\n<li><p><strong>“Early Adopters” or “Visionaries”</strong> see emerging technologies as opportunities to gain revolutionary capabilities before anyone else does. They represent about one-sixth of the population.</p>\n</li>\n<li><p><strong>The “Early Majority” or “Pragmatists”</strong> hope to benefit by keeping up with the state of the art, but they only buy proven products from reputable companies. They represent about a third of the population.</p>\n</li>\n<li><p><strong>The “Late Majority” or “Conservatives”</strong> don’t care about keeping up with the latest developments, but they don’t want to be left behind either. They tend to value simplicity, convenience, and affordability over performance. They make up about one-third of the population.</p>\n</li>\n<li><p><strong>The “Laggards” or “Skeptics”</strong> are innately skeptical of anything new. They never buy new technology if they can avoid it. They make up about one-sixth of the population.</p>\n</li>\n</ul>\n<p>Thus, when a disruptive innovation is introduced, the first customers are the “techies,” and so the initial market will be limited to a few percent of the overall population. Meanwhile, the majority of an established company’s customers will be members of the early and late majorities, who don’t become interested until the innovation has been refined and standardized.</p>\n</blockquote>\n<h5 id=\"3-disruptive-innovations-cant-demonstrate-potential-growth\">3. Disruptive Innovations Can’t Demonstrate Potential Growth</h5>\n<p>As Christensen points out, most established companies are under pressure to maintain a steady, positive rate of growth to keep their stock prices going up and thereby keep their stockholders happy. Therefore, when investing in a new product, companies are expected to promise substantial growth to justify their investment. However, even if a company recognizes that the market for a disruptive technology may grow as the technology matures, they can’t show their shareholders and investors market data on markets that don’t exist yet. This makes it hard for them to justify investing in innovations, because established companies are used to making strategic decisions based on sound market research.</p>\n<p>(Shortform note: Compounding the problem of predicting which innovations may offer long-term growth and which will not is the fact that not <em>every</em> disruptive technology grows to take over the mainstream market. In fact, the majority of disruptive innovations only flourish briefly in an early market made up of techies and early adopters, after which they <a href=\"https://www.shortform.com/app/book/crossing-the-chasm\" target=\"_blank\">stagnate and die</a> without making it into the mainstream market.)</p>\n<h5 id=\"4-disruptive-innovations-require-retooling\">4. Disruptive Innovations Require Retooling</h5>\n<p>Christensen asserts that established companies are often poorly positioned to produce or sell disruptive products because the new innovation takes a different approach, requiring different component parts or manufacturing methods. This, in turn, may require different supply networks. The company’s operations are tailored to its current products and thus are not optimized for new innovation. They may not even be able to accommodate it without an expensive overhaul.</p>\n<p>For example, suppose a horse trader wants to start dealing in automobiles. The horse trader already has connections with ranchers and breeders who supply him with wholesale horses, but he’ll need to develop new relationships with automobile manufacturers. He also has stables to keep his horses in, and barns to store hay, but he’ll have to remodel them to store automobiles and spare parts. He currently pays stable hands to feed and water the horses every day, but that won’t be necessary with automobiles, so he’ll have to re-think his daily routines and operating procedures to keep his employees productive.</p>\n<blockquote><p><strong>Retooling for Innovations in Sales Methods</strong></p>\n<p>This principle applies to practically all disruptive innovations, not just physical products. In <em><a href=\"https://www.shortform.com/app/book/the-challenger-sale\" target=\"_blank\">The Challenger Sale</a></em>, Mathew Dixon and Brent Adamson explain “solution selling,” a disruptive innovation in business-to-business sales: Instead of simply fulfilling purchase orders, you would <a href=\"https://www.shortform.com/app/book/the-challenger-sale/chapter-1#the-rise-of-solution-selling\" target=\"_blank\">sell the customer a complete solution</a> customized to their needs.</p>\n<p>But this changed the relationship between the customer and the salesperson. Instead of just delivering what the customer ordered, the sales rep had to assume a consultative role, challenging the customer to rethink their operations and showing them how they could benefit from a customized solution.</p>\n<p>Dixon and Adamson observed that relatively few sales reps were able to transition to solution selling, because they struggled to mentally retool their role, just like a factory struggling to retool its production lines for a different product.</p>\n</blockquote>\n<h5 id=\"5-disruptive-innovations-appear-down-market-of-established-products\">5. Disruptive Innovations Appear Down-Market of Established Products</h5>\n<p>Christensen argues that most companies tend to move up-market over time, appealing to increasingly demanding customers, and reaping higher profits from higher-end products as the company gains experience and refines its products. However, disruptive innovations usually start out down-market—they start out relatively simplistic and have to be priced at the lower end of the market spectrum.</p>\n<p>(Shortform note: Al Ries and Jack Trout offer additional insight on why disruptive innovations start out down-market. They assert that budget-friendly versions of new innovations tend to sell better, because customers see the technology as unproven. The lower cost <a href=\"https://www.shortform.com/app/book/positioning/1-page-summary\" target=\"_blank\">reduces the risk</a> of buying it.)</p>\n<p>Christensen points out that to adopt the new innovation, a company would have to shift its focus back toward the lower end of the market. This is financially problematic because of the lower profits per unit sold. It’s also culturally problematic, because it’s hard to give up the prestige that you’ve built up and start over.</p>\n<blockquote><p><strong>Cross-Examining Up-Market Mobility</strong></p>\n<p>Do companies really tend to drift up-market over time as Christensen says? It makes sense that, as you gain experience with making a particular product, you’ll figure out how to make it better. In principle your product should move up-market as it improves. However, in practice, the marketing concept of <em><a href=\"https://www.shortform.com/app/book/crossing-the-chasm/chapter-6\" target=\"_blank\">positioning</a></em> makes it difficult to move up-market or down-market.</p>\n<p>Your “positioning” is how customers see your product or brand. For example, do they see you as the quality leader in your industry? Or the cheap knock-off? Or the environmentally-friendly option? Or the version made especially for a particular gender, age group, or other demographic?</p>\n<p>Marketing experts assert that once customers have established your position on the market landscape, it is very <a href=\"https://www.shortform.com/app/book/positioning/1-page-summary#what-does-positioning-look-like\" target=\"_blank\">difficult to change</a> their perception of your product or brand. So if you start out selling entry-level products and customers recognize your place in the market, it may be difficult to move up-market, even if your quality improves over time.</p>\n</blockquote>\n<h3 id=\"what-you-can-do-to-adopt-disruptive-innovations-successfully\">What You Can Do to Adopt Disruptive Innovations Successfully</h3>\n<p>We’ve explained what disruptive innovations are, how they disrupt markets, and why they’re a problem for established companies. Now let’s examine Christensen’s advice on how established companies can overcome these problems.</p>\n<p>Christensen’s recommendation is to establish an organization that functions like a small start-up within the parent company. This works because, as we’ve seen, most of the problems that prevent established companies from investing in disruptive innovations are irrelevant to small start-ups.</p>\n<p>You can create the organization either as a spin-off of your existing operations, or by acquiring an existing start-up. Either way, Christensen stresses that the new organization must have adequate autonomy to explore and develop disruptive innovations. It will struggle if it has to compete for resources with the company’s sustaining-innovation projects, because it operates in an emerging market where there’s less opportunity for high-volume sales. It may also need to develop supply chains and operating procedures that are incompatible with those of the parent company.</p>\n<p>(Shortform note: In <em><a href=\"https://www.shortform.com/app/book/blue-ocean-strategy\" target=\"_blank\">Blue Ocean Strategy</a></em>, W. Chan Kim and Renée Mauborgne argue that emerging markets with less opportunity for high-volume sales are actually the most secure path to eventual high-volume sales, as a company’s best chance to grow and succeed is when it operates in an uncontested market—as it would when introducing a disruptive innovation. They argue that although you may have difficulty in developing new supply chains and operating procedures, as Christensen mentions, <a href=\"https://www.shortform.com/app/book/blue-ocean-strategy/part-2-4#establish-defensive-barriers\" target=\"_blank\">your competitors will also</a>. Thus, the challenges you might face as a start-up can end up being your strengths because they delay other players from entering the market.)</p>\n<p>Christensen also stresses the importance of budgeting for multiple attempts at marketing the disruptive innovation. Recall that start-ups often have to find the emergent market by trial and error, because there’s no actual data on markets that don’t exist yet, which makes them impossible to accurately forecast. You may have to explore several different applications or niche markets before you find one where the product takes off.</p>\n<blockquote><p><strong>Making Decisions Without Data</strong></p>\n<p>Moore echoes Christensen’s observation that emergent markets are hard to forecast because you can’t gather metrics on them yet. However, where Christensen advocates mitigating the uncertainty by budgeting for multiple attempts, Moore suggests <a href=\"https://www.shortform.com/app/book/crossing-the-chasm/chapter-4#step-1-generate-customer-profiles\" target=\"_blank\">using customer characterizations to guide your planning</a>.</p>\n<p>Each customer characterization is a description of a hypothetical customer, complete with information about their age, job, goals, and so on. These descriptions should be as realistic and as lifelike as possible. Once you’ve created profiles for the archetypical customers that might want your product, you use them to think through hypothetical purchasing scenarios: What does the customer’s current situation look like without your product? How would having your product solve her current problems? Is the improvement enough to compel her to buy?</p>\n<p>By anticipating how each hypothetical customer would behave in each scenario, you can assess which customer would be most likely to buy your product. According to Moore, this method works because your intuition is much better at predicting how a person would respond to a situation than it is at predicting the behavior of abstract entities like markets. And when you’re introducing a disruptive innovation, you have to make decisions based on informed intuition, because you don’t have enough data to make an analytical decision.</p>\n<p>Of course, these two methods are not mutually exclusive. When you’re operating in uncharted technological territory, it’s prudent to inform your intuition with customer characterizations <em>and</em> budget for multiple attempts whenever possible.</p>\n</blockquote>\n<div></div><blockquote><p><strong>Managing an Acquired Start-Up (Or Spin-Off)</strong></p>\n<p>Like Christensen, business and marketing consultant Regis McKenna points out that collaboration between a large, established company and a small, innovative start-up organization can be beneficial because of their different strengths and capabilities. However, he also points out that their differences can make collaboration difficult, so much so that many <a href=\"https://archive.org/details/registouchnewmar00mcke/page/70/mode/2up\" target=\"_blank\">attempts at collaboration end in bankruptcy</a> for one or both organizations.</p>\n<p>McKenna notes that collaboration can take the form of a joint venture, an acquisition, or any other agreement to work together on something. He doesn’t specifically mention collaboration between a parent company and a start-up-like spin-off organization within the company, but since this relationship is very similar to that of an acquired start-up, we infer that the same problems would apply.</p>\n<p>McKenna identifies two keys to effective collaboration between a start-up and an established company:</p>\n<p><strong>1. Adequate separation. </strong>Both organizations must retain enough autonomy that they each have their own supply lines, customers, and unique culture. He warns that without adequate separation, the culture and resources of the larger organization tend to crowd out those of the smaller start-up, such that the start-up’s strengths are lost.</p>\n<p>Christensen mirrors this idea by recommending that a start-up or spin-off have autonomy so that it can explore innovations without influence from the parent company. He also implicitly acknowledges McKenna’s advice by pointing out the problems that established companies have adopting disruptive innovations: Most of these are not a problem for a small start-up, but if the start-up gets absorbed into the established company (or is never sufficiently separate from it) then the established company’s problems will still apply.</p>\n<p><strong>2. Adequate communication.</strong> While maintaining autonomy, the two organizations also need to communicate enough that they both have a clear understanding of who is responsible for what. McKenna warns that when organizations try to collaborate without adequate communication, important tasks are often left undone because each org assumes the other is taking care of it.</p>\n<p>Christensen doesn’t address this, likely because he considered only collaboration between an <em>acquired</em> start-up or a spin-off organization, and doesn’t address collaboration between <em>separate companies</em>. We might infer that collaboration between separate companies more often fails due to inadequate communication, while collaboration between two organizations of the same parent company is more likely to fail due to inadequate separation.</p>\n</blockquote>\n"
  },
  {
    "cover_image": "https://media.shortform.com/covers/png/sex-at-dawn-cover.png",
    "title": "Sex at Dawn",
    "author": "Christopher Ryan and Cacilda Jethá",
    "tags": [
      "History",
      "Relationships",
      "Society/Culture"
    ],
    "url_slug": "sex-at-dawn",
    "html": "<p>In <em>Sex at Dawn</em>, Christopher Ryan and Cacilda Jethá argue that everything we think we know about prehistoric human sexuality is wrong. Contrary to popular belief, humans haven’t always formed monogamous pair bonds—instead, the authors say, <strong>prehistoric humans lived in foraging societies that encouraged casual sex with multiple mates</strong>. In their view, humans only reluctantly embraced monogamy about 10,000 years ago when we stopped foraging for food and started farming.</p>\n<p>In this guide, we’ll begin by covering the “standard narrative,” or widely accepted set of beliefs around human sexuality. Then, we’ll examine the logical flaws in the standard narrative, based on evidence from the sociosexual habits of great apes, observations of remote hunter-gatherer societies, and human biology. Finally, we’ll learn the new narrative of pre-agricultural human sexuality that Ryan and Jethá propose. Along the way, we’ll also see what other evolutionary psychologists and anthropologists have to say about Ryan and Jethá’s ideas.</p>\n<p>(Shortform note: <em>Sex at Dawn</em> was well-received by general readers, but <a href=\"https://en.wikipedia.org/wiki/Sex_at_Dawn#Scholarly_reception\" target=\"_blank\">many evolutionary psychologists and anthropologists were critical of Ryan and Jethá’s interpretation of the evidence</a>. In fact, one scholar wrote an entire book, <em><a href=\"https://www.amazon.com/Sex-Dusk-Lifting-Shiny-Wrapping/dp/1477697284\" target=\"_blank\">Sex at Dusk</a></em>, in which she uses the same scientific evidence as Ryan and Jethá to attempt to disprove their conclusions.)</p>\n<h3 id=\"part-1-the-standard-narrative\">Part 1: The Standard Narrative</h3>\n<p>Before we dive into the authors’ arguments, we need to understand the traditional description of evolutionary human sexuality—the “standard narrative”—that Ryan and Jethá argue against. The standard narrative describes the way men’s and women’s approaches to reproduction evolved over time. (Shortform note: In general, when Ryan and Jethá refer to “men” and “women,” we can infer that they’re specifically referring to cisgender people. <em>Sex at Dawn</em> was published in 2010 and does not explicitly mention transgender or intersex people.)</p>\n<p>According to the standard narrative, the human mating system works like this: If a man and a woman find each other desirable, they’ll form a long-term, monogamous bond (from which they’ll periodically escape for flings with other partners). This pairing offers women the security of access to resources and offers men the all-important certainty that they are their children’s biological father.</p>\n<p>(Shortform note: In <em><a href=\"https://www.shortform.com/app/book/mating-in-captivity\" target=\"_blank\">Mating in Captivity</a></em>, couples therapist Esther Perel argues that the nature of monogamy has continued to evolve beyond this narrative in that <a href=\"https://www.shortform.com/app/book/mating-in-captivity/chapter-9\" target=\"_blank\">modern monogamy is less of a practical exchange of resources and more of an expression of love and commitment to a partner</a>.)</p>\n<p>According to Ryan and Jethá, the standard narrative is an example of<strong> “Flinstonization,” or the tendency to use modern cultural mores to explain historical human behavior. </strong>For example, we frequently hear stories of women “settling” for partners who can provide financial security, even if they’re not a love match. According to the authors, we then mistakenly project the same expectation backward onto prehistoric women because we assume that “settling” must be an innate (rather than culturally-ingrained) behavior. However, prehistoric humans lived in very different social and physical environments than modern humans, so it’s unwise to assume they went through the same thought processes as modern humans in choosing whether or not to commit to a mate.</p>\n<p>(Shortform note: Flinstonization is a form of the “narrative fallacy.” In <em><a href=\"https://www.shortform.com/app/book/thinking-fast-and-slow\" target=\"_blank\">Thinking, Fast and Slow</a></em>, Daniel Kahneman describes the narrative fallacy as <a href=\"https://www.shortform.com/app/book/thinking-fast-and-slow/part-3\" target=\"_blank\">the human tendency to rearrange facts into consistent stories in order to make sense of the world</a>. When scientists Flinstonize the past, they’re essentially rearranging the evidence to fit the current cultural story.)</p>\n<h4 id=\"assumptions-of-the-standard-narrative\">Assumptions of the Standard Narrative</h4>\n<p>According to the authors, the standard narrative relies on a set of basic assumptions, which we’ll explore below. Each of these assumptions is based on the underlying idea that <strong>passing on one’s own genes is the ultimate motivation for all human beings. </strong></p>\n<p>(Shortform note: In <em><a href=\"https://www.shortform.com/app/book/the-selfish-gene\" target=\"_blank\">The Selfish Gene</a></em>, Richard Dawkins argues that <a href=\"https://www.shortform.com/app/book/the-selfish-gene/chapter-3\" target=\"_blank\">the motivation to reproduce happens at the level of each individual gene</a>, not the whole-organism level. That’s because, if individual organisms (such as humans) were motivated to reproduce their entire genetic codes, they wouldn’t focus on sexual reproduction, which only passes down <em>half</em> of their genes. In Dawkins’ view, the fact that humans are so focused on <em>sexual</em> reproduction disproves the idea that our objective is <em>genetic</em> reproduction.)</p>\n<h5 id=\"assumption-1-monogamy-is-natural\">Assumption 1: Monogamy Is “Natural.”</h5>\n<p>According to the standard narrative, monogamy is the natural result of men’s and women’s differing reproductive imperatives. This logic requires its own set of contributing assumptions:</p>\n<ul>\n<li><strong>Women’s libidos are naturally lower than men’s. </strong>Thus, in the standard narrative, a woman’s interest in sex is primarily economic: She grants a male partner exclusive sexual access and, in return, he provides resources exclusively to her and her children. (Shortform note: The assumption that women have naturally lower libidos than men may be due to the fact that <a href=\"https://www.goodtherapy.org/blog/when-urge-is-uneven-understanding-universe-of-sexual-desire-0206185\" target=\"_blank\">women experience spontaneous, unprompted sexual desire less frequently than men do</a>. However, research shows that women are more likely than men to experience “responsive desire,” or sexual arousal in response to physical stimulation. In other words, women don’t have <em>lower</em> libidos, they have <em>different</em> libidos.) </li>\n<li><strong>“Male parental investment,” or fathers’ investment in the survival of their children, is dependent on paternity certainty.</strong> According to the standard narrative, men are biologically driven to have as many children as possible and to ensure the survival of those children in order to perpetuate their genetics. Thus, men are motivated to protect and provide for <em>only </em>the children they are 100% positive are their biological offspring. (Shortform note: This idea, like the rest of the standard narrative, refers to our<em> most primal</em> human nature. Even scientists who support the standard narrative would likely agree that cultural patterns can sometimes trump these “natural” patterns—for example, in the case of step-fathers and adoptive fathers happily raising children they’re not biologically related to.) </li>\n</ul>\n<p>According to the authors, the standard narrative insists that monogamy is the only system that allows both men and women to meet these reproductive goals. However, the standard narrative also acknowledges that both men and women will be motivated to cheat on their partners if someone with more desirable genetic traits becomes available.</p>\n<p>(Shortform note: The motivation to meet reproductive goals is how the standard narrative explains the development of human monogamy. However, other researchers argue that <a href=\"https://scienceillustrated.com.au/blog/culture/the-origin-of-monogamy/\" target=\"_blank\">monogamy actually developed as a way for non-dominant males to gain access to females</a> without having to fight dominant males in the group. Instead of fighting, the non-dominant males began sharing their food with females—in return, females granted these males sexual exclusivity. Observation of this behavior led to the idea of monogamy as an exchange of resources. This model of resource-sharing also explains the development of infidelity—if a male came along with access to more or better food, it made sense for a female to abandon her previous mate for the new one.)</p>\n<h5 id=\"assumption-2-jealousy-differs-between-the-sexes\">Assumption 2: Jealousy Differs Between the Sexes</h5>\n<p>According to the authors, the standard narrative states that <strong>men naturally get jealous when their female partners are <em>sexually</em> intimate with other men, while women naturally get jealous when their male partners are <em>emotionally</em> intimate with other women.</strong> This is also a direct result of their competing agendas: Men need to know their children are their own, so they’re threatened by the idea of another man impregnating their partner. Meanwhile, women need to maintain access to men’s resources, so they’re threatened by the idea of another woman convincing their partner to divert his economic resources to <em>her</em> children.</p>\n<p>(Shortform note: Research shows that gender isn’t the only important factor determining how people experience jealousy. For example, one study found that <a href=\"https://www.researchgate.net/profile/Danielle-Zandbergen-2/publication/265855105_Culture_and_gender_differences_in_romantic_jealousy/links/60369f5a299bf1cc26ebe983/Culture-and-gender-differences-in-romantic-jealousy.pdf\" target=\"_blank\">people from collectivist cultures experienced higher rates of sexual jealousy</a>, but not emotional jealousy. The same study found that people who had been cheated on by a previous partner experienced higher rates of both sexual and emotional jealousy, regardless of their gender. Therefore, gender is likely just one of a host of factors that predict jealous reactions.)</p>\n<blockquote><p><strong>Is There Really One “Standard” Narrative? </strong></p>\n<p>We’ve discussed what the authors claim constitutes the standard narrative, but does a single “standard” narrative even exist? Researcher Emily Nagoski argues that <a href=\"https://web.archive.org/web/20140808123320/http://www.thedirtynormal.com/blog/2013/02/22/book-review-sex-at-dawn/\" target=\"_blank\">there’s no such thing as a standard scientific narrative</a>. Instead, Nagoski believes that what Ryan and Jethá are refuting is a <em>cultural </em>narrative that misrepresents the science behind it. However, when Nagoski asked <em>Sex at Dawn</em> co-author Christopher Ryan about this, he clarified that the “standard narrative” is meant to be scientific, not cultural.</p>\n<p>Nagoski argues that this definition makes Ryan and Jethá’s argument collapse on itself, because if the standard narrative were truly a scientific argument, it would be impossible to refute it using science as the authors do in <em>Sex at Dawn</em>. In other words, the authors are using science both to support and refute their own argument, which creates a contradiction.</p>\n<p>However, Nagoski’s argument assumes that evolutionary science is purely <em>objective</em>. In reality, <a href=\"https://www.nature.com/articles/nmeth.3798#:~:text=Scientific%20peer%20disagreement%20can%20be,on%20how%20to%20achieve%20it.\" target=\"_blank\">different scientists can, and do, disagree on how to interpret the same data</a> and use these disagreements to drive scientific discovery.</p>\n</blockquote>\n<h3 id=\"part-2-evidence-from-great-apes\">Part 2: Evidence From Great Apes</h3>\n<p>To refute the standard narrative, Ryan and Jethá use evidence from three sources: the sociosexual habits of great apes with close genetic links to humans, observations of remote hunter-gatherer societies, and human biology. In this section, we’ll begin by examining the evidence from the great apes: specifically, chimpanzees and bonobos, which are species with non-monogamous (or multimale-multifemale) mating systems.</p>\n<p>Let’s look at those mating systems in detail.</p>\n<h4 id=\"chimpanzees\">Chimpanzees</h4>\n<p>According to the authors, twentieth-century scientists thought chimpanzees were a nearly perfect model of ancient, unrestrained, “primal” humans because while they exhibit very human behaviors, they’re far less inhibited and more openly brutal than modern humans. These scientists also speculated that chimps’ approach to sex must represent ancient humans’ primal reproductive instincts: instincts that we still have, but repress.</p>\n<p>(Shortform note: This belief that chimp behavior—sexual or otherwise—is equivalent to primal human behavior is why, in <em><a href=\"https://shortform.com/app/book/the-chimp-paradox\" target=\"_blank\">The Chimp Paradox</a></em>, psychiatrist Steve Peters refers to humans’ most instinctive urges and emotional reactions as the “Inner Chimp” that must be controlled by the rational “Inner Human.”)</p>\n<p>The authors note that chimps are quite promiscuous, and female chimpanzees often mate multiple times per day with various males. However, sex for chimpanzees is almost exclusively reproductive—female chimps are only sexually active during the fertile period of their menstrual cycles. (Shortform note: While female chimps may <em>attempt</em> promiscuity while fertile, they’re not always successful. <a href=\"https://sciencing.com/chimpanzee-mating-habits-6703991.html\" target=\"_blank\">Aggressive male chimpanzees have been known to physically restrain ovulating females</a> to prevent them from mating with other males in the community.)</p>\n<h4 id=\"bonobos\">Bonobos</h4>\n<p>Ryan and Jethá note that, like chimpanzees, bonobos are famously promiscuous. However, for bonobos, sex serves important purposes beyond reproduction: It’s the social glue that holds the group together. This is evident in the fact that bonobos also engage in other forms of sexual intimacy to cement their bond, like kissing and looking into one another’s eyes while mating. (Shortform note: Bonobos’ non-reproductive approach to sex goes even further: Other researchers have observed bonobos engaging in oral sex and even <a href=\"https://www.nytimes.com/2016/09/13/science/bonobos-apes-matriarchy.html\" target=\"_blank\">making sex toys out of fallen tree branches</a>. Bonobos also <a href=\"http://www.bbc.com/earth/story/20160317-do-bonobos-really-spend-all-their-time-having-sex\" target=\"_blank\">frequently engage in female-female and male-male sexual contact</a>.)</p>\n<p>Crucially, bonobos don’t exhibit the same patterns of male aggression, jealousy, and attempts to control female sexuality that we see in humans. The authors believe this means that these social patterns are rooted in our modern culture, not ancient biology. (Shortform note: Some reviewers have questioned this conclusion because scientists <em>have </em>observed aggression in wild bonobos. According to anthropologist Ryan Ellsworth, <a href=\"https://www.researchgate.net/publication/230796627_The_myth_of_promiscuity_A_review_of_Lynn_Saxon_Sex_at_Dusk_Lifting_the_Shiny_Wrapping_from_Sex_at_Dawn\" target=\"_blank\">Ryan and Jethá gave a purposefully incomplete overview of the evidence from great apes in order to make their point</a>.)</p>\n<h4 id=\"from-great-apes-to-humans\">From Great Apes to Humans</h4>\n<p>While chimpanzees and bonobos—our two closest evolutionary cousins—have different approaches to sex and reproduction, they both practice the same multimale-multifemale mating system. Thus, the authors conclude that this is the most natural mating system for social primates—including humans.</p>\n<p>(Shortform note: In her review of <em>Sex at Dawn</em>, Emily Nagoski takes issue with this conclusion. She argues that <a href=\"https://web.archive.org/web/20140808123320/http://www.thedirtynormal.com/blog/2013/02/22/book-review-sex-at-dawn/\" target=\"_blank\">primate sexuality is meant to adapt to the social context</a>. In other words, for primates, sex can serve whatever purpose we need or want it to serve (such as reproduction, pleasure, or control) depending on our circumstances and relationships. That means that, while one mating system may be “natural” for bonobos and chimps, that doesn’t mean the same system is “natural” for humans, who have much more complex social systems.)</p>\n<h3 id=\"part-3-evidence-from-foraging-societies\">Part 3: Evidence From Foraging Societies</h3>\n<p>In addition to great apes, the authors also use evidence from modern foraging societies to support their arguments against the standard narrative. These societies are geographically isolated from other people and still practice the type of hunter-gatherer lifestyle that our prehistoric ancestors did. The authors present two aspects of modern foraging societies that cast doubt on the standard narrative: partible paternity and non-nuclear families.</p>\n<p>(Shortform note: <a href=\"https://en.wikipedia.org/wiki/Hunter-gatherer\" target=\"_blank\">There are about 30 modern foraging societies</a> that we know of. However, as of 2018, there were also roughly 100 <a href=\"https://bigthink.com/scotty-hendricks/there-are-more-than-100-uncontacted-tribes-in-the-world-who-are-they\" target=\"_blank\">uncontacted tribes</a> in the world. Because these tribes have avoided or rejected interactions with outsiders, we don’t know for sure whether they embrace the same hunter-gatherer lifestyle as early humans did.)</p>\n<h4 id=\"partible-paternity\">Partible Paternity</h4>\n<p>According to the authors, one way that modern foraging societies refute the standard narrative is through partible paternity, or the idea that more than one man can be the biological father to a child. This argument arises from the fact that people in some remote South American foraging societies understand pregnancy and conception differently than people in most other societies do: They think pregnancy is the result of ongoing deposits of semen rather than a single sex act. Therefore, the authors argue, pregnant women in these cultures will seek out a variety of men with different desirable genetic traits for sex because they believe that each man’s semen will contribute to the baby’s genetic heritage.</p>\n<p>(Shortform note: According to some anthropologists, <a href=\"https://www.pnas.org/content/pnas/107/45/19195.full.pdf\" target=\"_blank\">partible paternity beliefs exist along a spectrum</a> for people in remote South American foraging societies. That is, people in some such societies wholeheartedly believe in partible paternity, while people in other, similar societies believe a child can only have one biological father. Researchers note that many people in these societies  fall somewhere in the middle: They believe that partible paternity is theoretically possible, but that it’s better for a woman to reproduce with just one male partner.)</p>\n<p>As a result of this practice, everyone in the community sees <em>all </em>of these men as biological fathers of the resulting child. According to the authors, each of the men is therefore responsible for providing for that child—which ultimately increases the child’s chances of surviving to adulthood. The authors believe this illustrates an important point: While paternity certainty might be important for individual men in some societies, a lack of paternity certainty may be better for the society as a whole. When men don’t know which children are their own, they have a vested interest in providing for <em>all </em>the children in their social group.</p>\n<p>(Shortform note: There is a natural limitation to the idea that men who can’t identify their biological children will be more invested in <em>all </em>the children in their community: It only applies in small, isolated groups, <a href=\"https://www.nationalgeographic.org/encyclopedia/hunter-gatherer-culture/#:~:text=Hunter%2Dgatherer%20groups%20tended%20to,more%20than%20about%20100%20people.\" target=\"_blank\">like the bands prehistoric humans lived in</a>. In these groups, it’s likely that at least some of the children in the group are any given man’s biological offspring, giving men motivation to care for these children. However, in larger or more interconnected groups, there’s a much smaller chance of any one child being any particular man’s biological child, giving men less reason to invest in all the children they meet.)</p>\n<blockquote><p><strong>Is Partible Paternity Evidence of “Natural” Promiscuity?</strong></p>\n<p>Some reviewers have argued that <a href=\"https://www.researchgate.net/profile/Robert-Walker-13/publication/266615978_Relatedness_Co-residence_and_Shared_Fatherhood_among_Ache_Foragers_of_Paraguay/links/543574fb0cf2643ab98664bc/Relatedness-Co-residence-and-Shared-Fatherhood-among-Ache-Foragers-of-Paraguay.pdf\" target=\"_blank\">the mere existence of partible paternity isn’t enough to prove Ryan and Jethá’s point</a> about which mating system is most “natural” for humans. For example, one group of evolutionary anthropologists and psychologists argued that, “the existence of partible paternity in some societies does not prove that humans are naturally promiscuous any more so than the existence of monogamy in some societies proves that humans are naturally monogamous.”</p>\n<p>However, if <a href=\"https://web.archive.org/web/20140808123320/http://www.thedirtynormal.com/blog/2013/02/22/book-review-sex-at-dawn/\" target=\"_blank\">human sexuality develops in response to social context</a>—and if modern foraging societies are socially similar to ancient foraging societies—we can assume that ancient humans probably did have some concept of partible paternity and a similarly non-monogamous approach to sex.</p>\n</blockquote>\n<h4 id=\"non-nuclear-families\">Non-Nuclear Families</h4>\n<p>According to the authors, partible paternity contributes to another way that foraging societies refute the standard narrative: a lack of traditional nuclear families. In modern foraging societies, children often have both multiple fathers and multiple mother figures. While everyone in the group knows who an infant’s biological mother is, women in the mother’s extended family will also nurse the baby. As they grow, children are free to wander from house to house, where each adult will care for them as they would their own children. The result is a diffuse sense of parental responsibility: Each adult acts as a parent to each child in the community, regardless of any biological relationship between them, because community bonds outweigh individual parent-child bonds.</p>\n<p>Ryan and Jethá believe that this evidence—coupled with the fact that, historically, nuclear families have needed significant help from tax breaks, religious edicts, and marriage laws to survive—undermines the idea of “natural” nuclear families. In human prehistory, it would have been unthinkable for a couple and their children to try to fend for themselves without the help of the community.</p>\n<p>(Shortform note: The authors argue that prehistoric humans wouldn’t have relied on nuclear family arrangements because the task of raising and providing for children was too difficult for two parents to manage without outside help. However, this argument fails to take into account the fact that families can both be nuclear <em>and</em> receive outside help. For example, <a href=\"https://www.theatlantic.com/family/archive/2020/02/nuclear-family-multigenerational-cohousing-depaulo/606511/\" target=\"_blank\">it’s possible for nuclear families to live in close communities with their extended families</a>. In that case, parents get extra help taking care of their children, but they’re still recognized as the parents—the nuclear family still exists. For all we know, our ancestors could have had similar arrangements.)</p>\n<blockquote><p><strong>Foraging Societies Raise Happier, Healthier Children</strong></p>\n<p>As part of their argument, the authors cite evidence from a number of foraging societies, including the Ye’kwana (sometimes spelled Ye’kuana) people of modern-day Venezuela and Brazil. Ryan and Jethá only focused on the aspects of the Ye’kwana child-rearing methods beneficial to basic survival, but other researchers have noted that these methods also carry important psychological benefits.</p>\n<p>In the mid-twentieth century, American writer Jean Liedloff lived with the Ye’kwana for several years. Liedloff found that, in addition to a diffuse sense of caregiving responsibility, <a href=\"https://continuumconcept.org/summary\" target=\"_blank\">the Ye’kwana people employed a number of unique parenting practices</a>, including cosleeping and breastfeeding on demand. Liedloff observed that the Ye’kwana children were happier and more self-confident than their American counterparts.</p>\n<p>Liedloff concluded that these benefits were the result of raising children in alignment with their bodies’  evolved expectations. For example, before car seats and strollers existed, babies spent most of their time in the arms of a loving adult. Over time, <a href=\"https://cosleeping.nd.edu/frequently-asked-questions/#Q3\" target=\"_blank\">infants evolved to depend on this contact to help regulate their internal temperature and heart rate</a>. Liedloff believed that when evolutionary expectations like this aren’t met—for example, when babies spend most of their time in strollers, bouncers, or cribs instead of being held—children aren’t able to thrive.</p>\n<p>Furthermore, the lack of emphasis on nuclear families in societies like the Ye’kwana enables these evolutionarily sound parenting practices—when the whole community takes responsibility for childcare, there’s always someone available to hold the baby, even if the child’s mother and father are busy.</p>\n</blockquote>\n<h3 id=\"part-4-evidence-from-modern-human-biology\">Part 4: Evidence From Modern Human Biology</h3>\n<p>Lastly, the authors use several aspects of modern human biology as evidence for their argument that prehistoric humans lived in promiscuous societies. These include body-size dimorphism, the size and shape of the human testicles and penis, and female copulatory vocalization. Let’s explore each in more detail.</p>\n<h4 id=\"body-size-dimorphism\">Body-Size Dimorphism</h4>\n<p>Body-size dimorphism<strong> </strong>is<strong> </strong>the average difference in body size between males and females of the same species. According to the authors, the more males in a species have to compete over females, the bigger the body-size dimorphism. That’s because bigger males tend to win competitions for females and pass on their genes, so each generation of males gets a little bit bigger. On the other hand, when there’s no need to compete, the genes for body size remain equally distributed in the population, so each generation of males stays about the same size.</p>\n<p>In humans, adult men tend to be, on average, 10% to 20% bigger than adult women. The authors believe that’s a relatively small difference compared to other species like gorillas, in which males can be up to 100% larger than females. They argue that the relatively small body-size dimorphism in humans is evidence that, for most of human history, there was little need for males to compete for females—otherwise, modern men would be much larger.</p>\n<p>According to the authors, this lack of male social competition is indicative of a multimale-multifemale mating system, in which no one had exclusive sexual access to anyone else—no one was “taken”—so there were more potential partners available for sex at any given time. (To contrast, monogamous societies have slightly <em>more </em>male social competition, because there are a finite number of female partners available.)</p>\n<blockquote><p><strong>A Different Explanation for Body-Size Dimorphism</strong></p>\n<p>While most scientists now agree with Ryan and Jethá’s interpretation of body size dimorphism, that was not always the case. For example, in 1986, paleontologist Martin Pickford proposed that <a href=\"https://link.springer.com/article/10.1007/BF02437287\" target=\"_blank\">body-size dimorphism arose because men and women have different energy demands</a>.</p>\n<p>In Pickford’s view, pregnancy and lactation are so energy-intensive that women gradually evolved a smaller body size (smaller bodies require less energy to maintain basic functioning, so they can afford to divert more energy to having children). Men, on the other hand, don’t have to devote energy to pregnancy, so they can afford to spend <em>all </em>their energy maintaining a larger body. To support this argument, Pickford cites the fact that the combined weight of an average woman and her infant child is close to the weight of an average man, which implies that the combined energy needs of mother and child would be about equal to a man.</p>\n<p>Overall, this explanation for body-size dimorphism has a unique consequence. While scientists like Ryan and Jethá believe that male competition for mates <em>causes </em>body-size dimorphism, Pickford believes such competition is the <em>result </em>of the body-size dimorphism that arose from men and women’s differing energy needs: in other words, that males only started to fight for females shortly <em>after </em>and <em>because</em> they evolved to be bigger, rather than evolving to be bigger because they needed to fight.</p>\n<p>If Pickford’s argument is correct, it would mean that male social competition for mates (and the associated monogamous mating system) began at least four million years ago, shortly after men and women first evolved to have different average body sizes. This would undermine Ryan and Jethá’s argument that humans have only been competing for mates since the advent of agriculture (around 10,000 years ago) and lived in more peaceful polyamory until then.</p>\n</blockquote>\n<h4 id=\"testicle-size\">Testicle Size</h4>\n<p>According to the authors, in highly sexually competitive species (such as gorillas), males tend to have smaller testicles—since only the dominant male has sexual access to a given female, his sperm is guaranteed to be the one that reaches her egg, so his body needn’t worry about producing large amounts of it. However, in more promiscuous species where each female might have many mates, males have evolved over time to have larger testicles, resulting in more sperm production and a higher chance of being the one to impregnate a given female and pass on their genes. According to the authors, this “sperm competition” replaces social competition among males; if males don’t need to compete for sexual access to a given female (because she’s mating with all of them), producing higher concentrations of sperm increases an individual male’s chances of being the one to impregnate that female.</p>\n<p>(Shortform note: In <em><a href=\"https://www.amazon.com/Sex-Dusk-Lifting-Shiny-Wrapping/dp/1477697284\" target=\"_blank\">Sex at Dusk</a></em>, Lynn Saxon argues that the emphasis on sperm competition is part of Ryan and Jethá’s agenda to <a href=\"https://www.researchgate.net/publication/230796627_The_myth_of_promiscuity_A_review_of_Lynn_Saxon_Sex_at_Dusk_Lifting_the_Shiny_Wrapping_from_Sex_at_Dawn\" target=\"_blank\">remove active choice from the narrative of female mate selection</a>. In Saxon’s view, the authors of <em>Sex at Dawn</em> believe that women’s mate selection process is entirely unconscious (because when she gives sexual access to multiple partners, she can’t choose whose sperm ultimately fertilizes her egg) and should remain that way. In other words, Saxon believes Ryan and Jethá are arguing that women never did—nor should they now—<em>consciously</em> choose which men to mate with.)</p>\n<p>Human testicles, on average, are larger than those of polygynous (one male mating with multiple females) gorillas and smaller than those of promiscuous chimpanzees and bonobos. The authors believe this means that, until about 10,000 years ago, humans were a notably promiscuous species. However, with the advent of agriculture came monogamy and one-to-one sexual matches, which meant that even males with smaller testicles (who produced less sperm) had a chance of impregnating someone. Thus, the genes for reduced fertility were passed on and allowed to spread in the general population. The authors argue that this led to an overall reduced testicle size in the last 10,000 years. Therefore, they conclude that, until 10,000 years ago, humans lived in a non-competitive, multimale-multifemale mating system.</p>\n<p>(Shortform note: Ryan and Jethá’s argument is not universally accepted among evolutionary anthropologists. One reviewer, William Buckner, pointed out that <a href=\"https://quillette.com/2018/06/07/explaining-monogamy-vox/\" target=\"_blank\">human testicles (which weigh in at 34 grams on average) are much closer in size to gorillas’ (23 grams) than to chimps’</a> (149 grams) or bonobos’ (168 grams). Buckner argues that it’s unlikely that human testicles reduced from a size comparable to chimpanzees’ and bonobos’ to their current size in just 10,000 years—therefore, it’s more likely that humans evolved in polygynous groups, similar to gorillas.)</p>\n<h4 id=\"penis-shape-and-size\">Penis Shape and Size</h4>\n<p>According to the authors, the human penis is much larger than that of any great ape. It’s also uniquely shaped: Whereas animal penises typically have a tapered or curled end, the human penis has a flared glans. Ryan and Jethá believe this unique anatomy is the evolutionary result of thousands of years of humans living in promiscuous mating systems. Their argument is that this shape, combined with the thrusting motion of sex, is designed to create suction inside the vagina, which pulls the sperm of other males away from the female’s cervix. This suction only happens before ejaculation (at which point the penis changes shape), ensuring that a man’s own sperm isn’t similarly suctioned out. The authors claim this evolutionary adaptation only makes sense if humans evolved in a mating system where sperm competition was important—such as a multimale-multifemale mating system.</p>\n<p>(Shortform note: Other anthropologists have questioned Ryan and Jethá’s interpretation of the evidence. They argue not only that the shape of the human penis isn’t particularly unique in the animal kingdom, but also that its shape <a href=\"https://traditionsofconflict.com/blog/2018/6/7/the-human-penis-is-remarkably-boring\" target=\"_blank\">most closely resembles those of polygynous primates</a> rather than primates who live in multimale-multifemale mating systems.)</p>\n<h4 id=\"female-copulatory-vocalization-fcv\">Female Copulatory Vocalization (FCV)</h4>\n<p>Women tend to make more noise during sex than men—a phenomenon that scientists call “female copulatory vocalization,” or FCV. According to the authors, FCV is common in many species. Primatologists have observed that the more promiscuous a species is, the more likely women are to vocalize during sex.</p>\n<p>What does FCV have to do with promiscuity? The authors believe that these vocalizations are designed to attract more men, thereby increasing sperm competition and ensuring they pass on the best possible genes to their offspring. Thus, they argue, the presence of FCV in humans is more evidence that humans evolved as a highly promiscuous species.</p>\n<p>(Shortform note: A 2011 study found that, in humans, <a href=\"https://link.springer.com/article/10.1007/s10508-010-9632-1?source=post_page---------------------------\" target=\"_blank\">FCV might not be a completely unconscious response to orgasm</a>. While heterosexual women are most likely to orgasm during foreplay, they were most likely to vocalize intensely during vaginal penetration. This intensity reached a peak at the point of <em>male </em>orgasm. Thus, researchers concluded that FCV might have evolved as a partly conscious strategy to advertise the male orgasm to surrounding males. This might help to attract more male mates and therefore increase sperm competition, ensuring that only the male with the strongest sperm (and thus, presumably, the best genes) actually succeeds in getting the female pregnant.)</p>\n<h3 id=\"part-5-the-new-narrative\">Part 5: The New Narrative</h3>\n<p>Ultimately, Ryan and Jethá argue for a new narrative of human sexual evolution to replace the standard narrative. Their new narrative is based on the assumption that, before the advent of agriculture, humans lived in multimale-multifemale mating systems,<em> not </em>polygynous harems or monogamous pair bonds.</p>\n<p>(Shortform note: It’s important to note that this is just <em>one </em>possible theory of many. As we’ve explored, there isn’t a universally agreed-upon narrative—other researchers have come to different conclusions despite using the same evidence as Ryan and Jethá. For example, recall that the authors use average human testicle size as evidence that humans evolved in a multimale-multifemale mating system, but <a href=\"https://quillette.com/2018/06/07/explaining-monogamy-vox/\" target=\"_blank\">William Buckner used the same data as evidence that humans evolved in a polygynous mating system</a>.)</p>\n<p>This underlying assumption gives rise to a set of new conclusions about the nature of human sexuality:</p>\n<h4 id=\"conclusion-1-monogamy-isnt-natural\">Conclusion 1: Monogamy Isn’t “Natural”</h4>\n<p>According to the authors, monogamy is extremely rare in the natural world, occurring in just 3% of all mammals. Furthermore, among humans, adultery is a common occurrence in every culture around the world, even in spite of the brutal punishments that some societies inflict on adulterers. In Ryan and Jethá’s view, the only reason anyone would risk such punishment is to satisfy a deep evolutionary urge. The authors conclude that monogamy is thus not “natural” at all, contrary to the standard narrative.</p>\n<p>(Shortform note: The authors use the ubiquity of adultery as evidence against the standard narrative. However, the standard narrative does account for adultery—it acknowledges that it makes sense for both men and women to stray when a genetically superior mate comes along. Thus, the fact that adultery is common may not actually be an effective argument against the standard narrative.)</p>\n<p>What should we do with this information? The authors assert that they’re not out to promote any particular lifestyle or destroy monogamy as we know it—they merely aim to present the facts and let the reader decide how to respond to them. As a starting point, they recommend questioning the cultural rules around monogamy and potentially exploring whether some version of consensual non-monogamy might be a better fit for your relationship.</p>\n<p>(Shortform note: Ryan and Jethá don’t go into detail about the practice of consensual non-monogamy, or what many people call “ethical non-monogamy.” According to experts, <a href=\"https://www.verywellmind.com/what-is-ethical-non-monogamy-5176515\" target=\"_blank\">ethical non-monogamy is different from cheating because everyone in the relationship must consent to the arrangement in advance</a>. These relationships are “ethical” because they are rooted in trust and consent, not secrecy and betrayal.)</p>\n<h4 id=\"conclusion-2-jealousy-is-socially-constructed\">Conclusion 2: Jealousy Is Socially Constructed</h4>\n<p>According to the authors, some scientists argue that humans are naturally, universally jealous creatures and therefore could never have lived in non-monogamous societies.</p>\n<p>Ryan and Jethá believe that this reasoning is fundamentally illogical. They argue that jealousy stems from insecurity, and insecurity stems from the fear that there is not enough of something (like love or material resources) to go around. Therefore, in non-monogamous societies where no one is expected to rely on just one other person for love, sex, and resources, jealousy is reduced because there are always others who can provide what we need. Jealousy is thus a <em>result</em> of our cultural emphasis on monogamy rather than a <em>cause</em> of it.</p>\n<p>(Shortform note: The authors’ logic implies that jealousy should be less of an issue for people in polyamorous relationships because there is more than one person available to meet their needs. However, many people in polyamorous relationships report that <a href=\"https://www.insider.com/how-polyamorous-people-cope-with-jealousy-in-relationships-2020-2#jason-boyd-33-said-acknowledging-jealous-feelings-rather-than-ignoring-them-helps-1\" target=\"_blank\">jealousy is still very present—it’s just something they actively manage with intentional communication</a>. It’s difficult to tell whether jealousy arises in these cases because it’s a human universal or because people in modern polyamorous relationships were raised in cultures that normalized jealousy.)</p>\n"
  },
  {
    "cover_image": "https://media.shortform.com/covers/png/the-hero-with-a-thousand-faces-cover.png",
    "title": "The Hero with a Thousand Faces",
    "author": "Joseph Campbell",
    "tags": [
      "History",
      "Psychology",
      "Society/Culture"
    ],
    "url_slug": "the-hero-with-a-thousand-faces",
    "html": "<p><em>The Hero With a Thousand Faces </em>is a journey through the world’s mythological traditions, from the ancient Egyptians, to the Romans, the Hindu and Buddhist legends of the east, and the folk-tales and foundation myths of the indigenous peoples of the Americas and Oceania.</p>\n<p>It explores the common themes and story elements that define the world’s mythologies—though cultures are separated by vast gulfs of space and time,<strong> they all tell their stories in similar ways, using the same essential mythological template: the hero’s journey</strong>.</p>\n<h3 id=\"the-heros-journey\">The Hero’s Journey</h3>\n<p>The archetypal myth is that of the hero’s journey, which details the exploits of an exalted figure such as a legendary warrior or king. But the hero can also start out as an obscure figure of humble origins, on the fringes of society. Frequently, this hero is born to lowly circumstances in a remote corner of the world and is the product of immaculate conception and virgin birth. Thus,<strong> they start out with some essential element of the gods already inside them. </strong></p>\n<p>The hero sets out on a journey to acquire some object or attain some sort of divine wisdom. This can be something material (like Arthur’s quest for the Holy Grail) or something with far greater spiritual weight (like the Buddha’s journey to find ultimate enlightenment). The hero undergoes great trials and tribulations during the course of their quest, undergoes a spiritual (and sometimes literal) death and rebirth, and transforms into an entirely new being. They gain new powers, and with those powers, achieve their goal—they receive the ultimate boon. They then return home to share this heavenly reward with their people—and in doing so, redeem all mankind.</p>\n<p>Although the hero’s journey is often filled with daring exploits, the slaying of fantastical monsters, and unions with strange and beautiful goddesses, <strong>it is at heart a deeply introspective and inward-looking adventure,</strong> one with profound spiritual and psychological implications. Through their arduous trial, the hero learns new things about himself or herself and <strong>discovers hidden strengths that were dormant within them the entire time</strong>—in fairy tales, this is often made literal by the revelation of the hero to have been “the Chosen One” or “the King’s son.” These new (but latent) powers enable a thorough transformation of the hero’s outward being and psyche.</p>\n<p>When viewed this way, mythology is deeply egalitarian. It tells us who we are and the rewards that await us if we would only set aside our focus on the day-to-day humdrum of life and embrace the hero’s journey. <strong>The hero, far from being just a literary character of long-dead civilizations, symbolizes the great godly potential within all of us.</strong></p>\n<p><em>The Hero With a Thousand Faces </em>breaks down this mythological template even further and also explores the creation and destruction stories that mankind has told since before the age of recorded history, from cultures all over the world. A few key themes emerge.</p>\n<h3 id=\"the-monomyth\">The Monomyth</h3>\n<p><strong>The core structure of mythology is called the <em>monomyth. </em></strong>It involves three rites of passage—separation, initiation, and return. From the myths of the ancient Egyptians and the medieval Arthurian legend to the folk-tales of the native Maoris of New Zealand, the pattern of the hero’s journey usually follows this cycle: a separation from the world he or she has always known (embarking on the quest), gaining some spiritual or other-worldly power, and a return in which they share the boon of the new power with humanity.</p>\n<p><strong>There are familiar beats throughout world-legend</strong>—the call to action; the initial reluctance of the hero; the aid of a supernatural helper; the crossing of the threshold into the world of the unknown; union with the mother-goddess; the slaying of the father-god; the return to the land of the living; and the sharing of the ultimate boon.</p>\n<h3 id=\"creation-and-destruction\">Creation and Destruction</h3>\n<p><strong>Myths also point us to our place in the cosmos, our role in the great movement of the universe</strong>. Just as the monomyth shows the death, birth, and transformation of the individual in the form of the hero, so does mythology show the workings of all time and space—<strong>the origin story of the universe, and the means by which it will be destroyed and rebuilt</strong>. This is often represented as a universe without end, a universal round.</p>\n<p>In a version of this cycle told among the Aztecs of pre-Columbian Mexico, each of the four elements—water, earth, air, and fire—in their turn marked the end of an age of the world:</p>\n<ul>\n<li>the age of water ended in a flood (flood-myths are a common feature of mythological tradition)</li>\n<li>the age of earth culminated in an earthquake</li>\n<li>the age of wind finished with destruction by wind, or hurricane</li>\n<li>and the (present) age of fire would be brought to an end by flames. </li>\n</ul>\n<p>In the cosmogonic cycle of the Jains, eternity is represented as a spoked wheel, with each spoke representing one of the endlessly repeated ages of the universe, continuing in a permanent cycle.</p>\n<h3 id=\"psychological-journey\">Psychological Journey</h3>\n<p><strong>Myths are a society’s <em>outward </em>manifestations of inner conflicts and desires</strong>—they represent the expression of <em>unconscious </em>fears and desires. Here are common elements of myths that relate to psychological tensions or needs:</p>\n<ul>\n<li>The hero often first refuses the call to adventure. In psychoanalysis terms, this reflects the clinging to infantile needs for security. The mother and father are the figures preventing true growth and transformation.</li>\n<li>Once set off on an adventure, the hero encounters a point where they are further away from the world of comfort and familiarity than they have ever been before. This aspect of the heroic monomyth parallels the dangers and uncertainties of growing out of childhood and away from the protection of one’s parents. </li>\n<li>The hero often encounters goddesses, taking the form either of beauty and the feminine ideal, or of a witch who attempts to harm the hero. These figures represent the need to balance 1) our need for the love and protection of our parents (especially our mothers) with 2) our concurrent need to grow up and become independent adults.</li>\n<li>The hero also often encounters a father-god figure whom the hero must either overcome or reconcile with. In Freudian terms, this echoes the psychological rivalry that children feel toward their fathers. The father is the original intruder who enters the infant’s life after the serenity and union with the mother (goddess) in utero.</li>\n<li>After conquering their fears, the hero at last achieves their long-sought enlightenment. They have shattered the bounds of consciousness and reached a divine state. This teaches us that this power lives within us all—we achieve it through our own herohood.</li>\n</ul>\n<p>In modern times, this need to express unconscious desires is filled by the psychoanalyst, who analyzes and interprets dreams (a pure expression of the unconscious) and gives them meaning and structure. This is, in fact, a deeply ancient and profoundly mythic function— the psychoanalyst, like the medicine man and bard of old, helps us gain a deeper understanding of ourselves, our world, and our relationship to the cosmos. When we open ourselves up on the therapist’s couch, we are going into the furthest corners of the mind—we are, in effect, undergoing our own hero’s journey.</p>\n<h3 id=\"the-function-of-mythology-today\">The Function of Mythology Today</h3>\n<p>Unlike the ancients, we do not have the benefit of allegory and mythology to help us make sense of the bubbling up of our subconscious. As a secular, rational society, we increasingly lack the language to process this—psychoanalysis may be the closest thing, but it is not a substitute for the power of mythology and religion. Indeed, <strong>we have rationalized and argued our gods away</strong>.</p>\n<p>With the coming of secularization and rationalization, supernatural elements are often played down or meant to be interpreted simply as allegory or instructive fable. It is easy for this to happen to myths in modern, science-driven society, because it is easy to prove that the myths aren’t literally “true.” As history, biography, and science, mythology is obviously nonsense. But to make this observation is to miss the point about what myths are and what purpose they serve for the human experience. They are about the endless journey of the soul, the adventure into the furthest recesses of the self.</p>\n<p>It is only through studying these ancient soothsayers and shamans and the dead gods they once worshipped that we can truly grasp our fullest humanity.</p>\n<p>Mythology is still relevant. It binds us closer and provide us with a shared sense of community. Though we may lead atomized lives as husbands, wives, sons, daughters, professionals, and members of this or that nationality, we are bound together through shared myths. The ceremonies that derive from mythology, those of birth, initiation, marriage and death, remind us that <strong>we are part of something much larger than ourselves</strong>. We are only a cell, an organ of a much larger being. This is as true for us as it was for the ancients. Like Odysseus, like the Buddha, like Cuchulainn, great marvels and unfathomable transformations await the modern hero who heeds the mythic call.</p>\n"
  }
]